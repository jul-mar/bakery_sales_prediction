{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier werden alle Daten aus verschiedenen Quellen zu einem großen Dataframe zusammen gefasst. Das sind zum einen die Daten welche uns vom Kurs zu Verfügung gestellt wurden, als auch die daten welche wir selber noch gefunden haben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#packete für data handling laden\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Datum  Warengruppe      Umsatz\n",
      "0  2013-07-01            1  148.828353\n",
      "1  2013-07-02            1  159.793757\n",
      "2  2013-07-03            1  111.885594\n",
      "3  2013-07-04            1  168.864941\n",
      "4  2013-07-05            1  171.280754\n",
      "           Datum  Warengruppe     Umsatz\n",
      "9329  2017-12-21            6  87.471228\n",
      "9330  2017-12-22            6  71.911652\n",
      "9331  2017-12-23            6  84.062223\n",
      "9332  2017-12-24            6  60.981969\n",
      "9333  2017-12-27            6  34.972644\n",
      "(9334, 3)\n"
     ]
    }
   ],
   "source": [
    "#Einlesen der Basisidaten, welche wir zur verfügung gestellt bekommen haben\n",
    "\n",
    "#Umsatzdaten\n",
    "umsatz = pd.read_csv(\"/workspaces/bakery_sales_prediction/0_DataPreparation/umsatzdaten_gekuerzt.csv\")\n",
    "print(umsatz.head())\n",
    "print(umsatz.tail())\n",
    "print(umsatz.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Datum      Umsatz  Warengruppe_1  Warengruppe_2  Warengruppe_3  \\\n",
      "0  2013-07-01  148.828353              1              0              0   \n",
      "1  2013-07-02  159.793757              1              0              0   \n",
      "2  2013-07-03  111.885594              1              0              0   \n",
      "3  2013-07-04  168.864941              1              0              0   \n",
      "4  2013-07-05  171.280754              1              0              0   \n",
      "\n",
      "   Warengruppe_4  Warengruppe_5  Warengruppe_6  \n",
      "0              0              0              0  \n",
      "1              0              0              0  \n",
      "2              0              0              0  \n",
      "3              0              0              0  \n",
      "4              0              0              0  \n"
     ]
    }
   ],
   "source": [
    "#Warengruppen one hot encoden\n",
    "\n",
    "umsatz = pd.get_dummies(umsatz, columns=['Warengruppe'])\n",
    "umsatz = umsatz * 1\n",
    "print(umsatz.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Datum  Temp_Very_Cold  Temp_Cold  Temp_Mild  Temp_Warm  Temp_Hot  \\\n",
      "0  2012-01-01               0          1          0          0         0   \n",
      "1  2012-01-02               0          1          0          0         0   \n",
      "2  2012-01-03               0          1          0          0         0   \n",
      "3  2012-01-04               0          1          0          0         0   \n",
      "4  2012-01-05               0          1          0          0         0   \n",
      "\n",
      "   Cloud_Clear  Cloud_Partly_Cloudy  Cloud_Cloudy  Wind_Light  Wind_Moderate  \\\n",
      "0            0                    0             1           0              1   \n",
      "1            0                    0             1           0              1   \n",
      "2            0                    0             1           0              0   \n",
      "3            0                    1             0           0              0   \n",
      "4            0                    0             1           0              0   \n",
      "\n",
      "   Wind_Strong  Weather_Good  Weather_Light_Issues  Weather_Moderate  \\\n",
      "0            0             0                     1                 0   \n",
      "1            0             0                     1                 0   \n",
      "2            1             0                     1                 0   \n",
      "3            1             0                     0                 0   \n",
      "4            1             0                     0                 0   \n",
      "\n",
      "   Weather_Severe  \n",
      "0               0  \n",
      "1               0  \n",
      "2               0  \n",
      "3               1  \n",
      "4               1  \n",
      "           Datum  Temp_Very_Cold  Temp_Cold  Temp_Mild  Temp_Warm  Temp_Hot  \\\n",
      "2765  2019-07-28               0          0          0          1         0   \n",
      "2766  2019-07-29               0          0          0          0         1   \n",
      "2767  2019-07-30               0          0          0          1         0   \n",
      "2768  2019-07-31               0          0          0          1         0   \n",
      "2769  2019-08-01               0          0          0          1         0   \n",
      "\n",
      "      Cloud_Clear  Cloud_Partly_Cloudy  Cloud_Cloudy  Wind_Light  \\\n",
      "2765            0                    1             0           0   \n",
      "2766            0                    0             1           1   \n",
      "2767            0                    0             1           1   \n",
      "2768            0                    0             1           1   \n",
      "2769            0                    1             0           0   \n",
      "\n",
      "      Wind_Moderate  Wind_Strong  Weather_Good  Weather_Light_Issues  \\\n",
      "2765              1            0             0                     0   \n",
      "2766              0            0             0                     1   \n",
      "2767              0            0             0                     1   \n",
      "2768              0            0             0                     1   \n",
      "2769              1            0             0                     1   \n",
      "\n",
      "      Weather_Moderate  Weather_Severe  \n",
      "2765                 1               0  \n",
      "2766                 0               0  \n",
      "2767                 0               0  \n",
      "2768                 0               0  \n",
      "2769                 0               0  \n",
      "(2770, 16)\n"
     ]
    }
   ],
   "source": [
    "#Wetterdaten\n",
    "wetter = pd.read_csv(\"/workspaces/bakery_sales_prediction/0_DataPreparation/07_Wetter/07_wetter.csv\")\n",
    "print(wetter.head())\n",
    "print(wetter.tail())\n",
    "print(wetter.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Datum      Umsatz  Warengruppe_1  Warengruppe_2  Warengruppe_3  \\\n",
      "0  2013-07-01  148.828353              1              0              0   \n",
      "1  2013-07-02  159.793757              1              0              0   \n",
      "2  2013-07-03  111.885594              1              0              0   \n",
      "3  2013-07-04  168.864941              1              0              0   \n",
      "4  2013-07-05  171.280754              1              0              0   \n",
      "\n",
      "   Warengruppe_4  Warengruppe_5  Warengruppe_6  Temp_Very_Cold  Temp_Cold  \\\n",
      "0              0              0              0               0          0   \n",
      "1              0              0              0               0          0   \n",
      "2              0              0              0               0          0   \n",
      "3              0              0              0               0          0   \n",
      "4              0              0              0               0          0   \n",
      "\n",
      "   ...  Cloud_Clear  Cloud_Partly_Cloudy  Cloud_Cloudy  Wind_Light  \\\n",
      "0  ...            0                    0             1           0   \n",
      "1  ...            0                    1             0           0   \n",
      "2  ...            0                    0             1           1   \n",
      "3  ...            0                    0             1           1   \n",
      "4  ...            0                    1             0           0   \n",
      "\n",
      "   Wind_Moderate  Wind_Strong  Weather_Good  Weather_Light_Issues  \\\n",
      "0              1            0             0                     1   \n",
      "1              1            0             0                     1   \n",
      "2              0            0             0                     1   \n",
      "3              0            0             0                     1   \n",
      "4              1            0             0                     1   \n",
      "\n",
      "   Weather_Moderate  Weather_Severe  \n",
      "0                 0               0  \n",
      "1                 0               0  \n",
      "2                 0               0  \n",
      "3                 0               0  \n",
      "4                 0               0  \n",
      "\n",
      "[5 rows x 23 columns]\n",
      "(9334, 23)\n"
     ]
    }
   ],
   "source": [
    "#nun können die Umsatzdaten mit den Wetterdaten zusammengeführt werden\n",
    "#merge the data\n",
    "data = pd.merge(umsatz,wetter, how='left', on='Datum')\n",
    "print(data.head())\n",
    "print(data.shape)\n",
    "\n",
    "#es werden nur die Zeilen behalten, für welche es Umsatzdaten gibt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Datum  KielerWoche\n",
      "0  2012-06-16            1\n",
      "1  2012-06-17            1\n",
      "2  2012-06-18            1\n",
      "3  2012-06-19            1\n",
      "4  2012-06-20            1\n",
      "         Datum  KielerWoche\n",
      "67  2019-06-26            1\n",
      "68  2019-06-27            1\n",
      "69  2019-06-28            1\n",
      "70  2019-06-29            1\n",
      "71  2019-06-30            1\n",
      "(72, 2)\n"
     ]
    }
   ],
   "source": [
    "#Kieler Woche Daten\n",
    "kiwo = pd.read_csv(\"/workspaces/bakery_sales_prediction/0_DataPreparation/kiwo.csv\")\n",
    "print(kiwo.head())\n",
    "print(kiwo.tail())\n",
    "print(kiwo.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Datum      Umsatz  Warengruppe_1  Warengruppe_2  Warengruppe_3  \\\n",
      "0  2013-07-01  148.828353              1              0              0   \n",
      "1  2013-07-02  159.793757              1              0              0   \n",
      "2  2013-07-03  111.885594              1              0              0   \n",
      "3  2013-07-04  168.864941              1              0              0   \n",
      "4  2013-07-05  171.280754              1              0              0   \n",
      "\n",
      "   Warengruppe_4  Warengruppe_5  Warengruppe_6  Temp_Very_Cold  Temp_Cold  \\\n",
      "0              0              0              0               0          0   \n",
      "1              0              0              0               0          0   \n",
      "2              0              0              0               0          0   \n",
      "3              0              0              0               0          0   \n",
      "4              0              0              0               0          0   \n",
      "\n",
      "   ...  Cloud_Partly_Cloudy  Cloud_Cloudy  Wind_Light  Wind_Moderate  \\\n",
      "0  ...                    0             1           0              1   \n",
      "1  ...                    1             0           0              1   \n",
      "2  ...                    0             1           1              0   \n",
      "3  ...                    0             1           1              0   \n",
      "4  ...                    1             0           0              1   \n",
      "\n",
      "   Wind_Strong  Weather_Good  Weather_Light_Issues  Weather_Moderate  \\\n",
      "0            0             0                     1                 0   \n",
      "1            0             0                     1                 0   \n",
      "2            0             0                     1                 0   \n",
      "3            0             0                     1                 0   \n",
      "4            0             0                     1                 0   \n",
      "\n",
      "   Weather_Severe  KielerWoche  \n",
      "0               0          NaN  \n",
      "1               0          NaN  \n",
      "2               0          NaN  \n",
      "3               0          NaN  \n",
      "4               0          NaN  \n",
      "\n",
      "[5 rows x 24 columns]\n",
      "(9334, 24)\n",
      "        Datum      Umsatz  Warengruppe_1  Warengruppe_2  Warengruppe_3  \\\n",
      "0  2013-07-01  148.828353              1              0              0   \n",
      "1  2013-07-02  159.793757              1              0              0   \n",
      "2  2013-07-03  111.885594              1              0              0   \n",
      "3  2013-07-04  168.864941              1              0              0   \n",
      "4  2013-07-05  171.280754              1              0              0   \n",
      "\n",
      "   Warengruppe_4  Warengruppe_5  Warengruppe_6  Temp_Very_Cold  Temp_Cold  \\\n",
      "0              0              0              0               0          0   \n",
      "1              0              0              0               0          0   \n",
      "2              0              0              0               0          0   \n",
      "3              0              0              0               0          0   \n",
      "4              0              0              0               0          0   \n",
      "\n",
      "   ...  Cloud_Partly_Cloudy  Cloud_Cloudy  Wind_Light  Wind_Moderate  \\\n",
      "0  ...                    0             1           0              1   \n",
      "1  ...                    1             0           0              1   \n",
      "2  ...                    0             1           1              0   \n",
      "3  ...                    0             1           1              0   \n",
      "4  ...                    1             0           0              1   \n",
      "\n",
      "   Wind_Strong  Weather_Good  Weather_Light_Issues  Weather_Moderate  \\\n",
      "0            0             0                     1                 0   \n",
      "1            0             0                     1                 0   \n",
      "2            0             0                     1                 0   \n",
      "3            0             0                     1                 0   \n",
      "4            0             0                     1                 0   \n",
      "\n",
      "   Weather_Severe  KielerWoche  \n",
      "0               0            0  \n",
      "1               0            0  \n",
      "2               0            0  \n",
      "3               0            0  \n",
      "4               0            0  \n",
      "\n",
      "[5 rows x 24 columns]\n",
      "(9334, 24)\n"
     ]
    }
   ],
   "source": [
    "# und nun noch die Kieler Woche Daten hinzufügen\n",
    "data = pd.merge(data, kiwo, how='left', on='Datum')\n",
    "print(data.head())\n",
    "print(data.shape)\n",
    "\n",
    "#im moment steht da eine 1.0 wenn es Kieler Woche ist und eine NaN wenn nicht. Das ändern wir jetzt\n",
    "data['KielerWoche'] = data['KielerWoche'].fillna(0)\n",
    "data['KielerWoche'] = data['KielerWoche'].astype(int)\n",
    "print(data.head())\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Datum  Montag  Dienstag  Mittwoch  Donnerstag  Freitag  Samstag  \\\n",
      "0  2013-07-01       1         0         0           0        0        0   \n",
      "1  2013-07-02       0         1         0           0        0        0   \n",
      "2  2013-07-03       0         0         1           0        0        0   \n",
      "3  2013-07-04       0         0         0           1        0        0   \n",
      "4  2013-07-05       0         0         0           0        1        0   \n",
      "\n",
      "   Sonntag  \n",
      "0        0  \n",
      "1        0  \n",
      "2        0  \n",
      "3        0  \n",
      "4        0  \n",
      "        Datum      Umsatz  Warengruppe_1  Warengruppe_2  Warengruppe_3  \\\n",
      "0  2013-07-01  148.828353              1              0              0   \n",
      "1  2013-07-02  159.793757              1              0              0   \n",
      "2  2013-07-03  111.885594              1              0              0   \n",
      "3  2013-07-04  168.864941              1              0              0   \n",
      "4  2013-07-05  171.280754              1              0              0   \n",
      "\n",
      "   Warengruppe_4  Warengruppe_5  Warengruppe_6  Temp_Very_Cold  Temp_Cold  \\\n",
      "0              0              0              0               0          0   \n",
      "1              0              0              0               0          0   \n",
      "2              0              0              0               0          0   \n",
      "3              0              0              0               0          0   \n",
      "4              0              0              0               0          0   \n",
      "\n",
      "   ...  Weather_Moderate  Weather_Severe  KielerWoche  Montag  Dienstag  \\\n",
      "0  ...                 0               0            0       1         0   \n",
      "1  ...                 0               0            0       0         1   \n",
      "2  ...                 0               0            0       0         0   \n",
      "3  ...                 0               0            0       0         0   \n",
      "4  ...                 0               0            0       0         0   \n",
      "\n",
      "   Mittwoch  Donnerstag  Freitag  Samstag  Sonntag  \n",
      "0         0           0        0        0        0  \n",
      "1         0           0        0        0        0  \n",
      "2         1           0        0        0        0  \n",
      "3         0           1        0        0        0  \n",
      "4         0           0        1        0        0  \n",
      "\n",
      "[5 rows x 31 columns]\n",
      "(9334, 31)\n"
     ]
    }
   ],
   "source": [
    "#Wochentage hinzugefügen\n",
    "wochentag = pd.read_csv(\"/workspaces/bakery_sales_prediction/0_DataPreparation/09_Wochentage/Wochentage.csv\")\n",
    "print(wochentag.head())\n",
    "\n",
    "#mergen der Daten\n",
    "data = pd.merge(data, wochentag, how='left', on='Datum')\n",
    "print(data.head())\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun haben wir alle Basisidaten zusammen geführt. Jetzt können noch die Variablen, welche wir selber gesucht haben hinzugefügt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Datum    VPI\n",
      "0  2013-07-01  0.052\n",
      "1  2013-07-02  0.052\n",
      "2  2013-07-03  0.052\n",
      "3  2013-07-04  0.052\n",
      "4  2013-07-05  0.052\n",
      "(2222, 2)\n",
      "        Datum      Umsatz  Warengruppe_1  Warengruppe_2  Warengruppe_3  \\\n",
      "0  2013-07-01  148.828353              1              0              0   \n",
      "1  2013-07-02  159.793757              1              0              0   \n",
      "2  2013-07-03  111.885594              1              0              0   \n",
      "3  2013-07-04  168.864941              1              0              0   \n",
      "4  2013-07-05  171.280754              1              0              0   \n",
      "\n",
      "   Warengruppe_4  Warengruppe_5  Warengruppe_6  Temp_Very_Cold  Temp_Cold  \\\n",
      "0              0              0              0               0          0   \n",
      "1              0              0              0               0          0   \n",
      "2              0              0              0               0          0   \n",
      "3              0              0              0               0          0   \n",
      "4              0              0              0               0          0   \n",
      "\n",
      "   ...  Weather_Severe  KielerWoche  Montag  Dienstag  Mittwoch  Donnerstag  \\\n",
      "0  ...               0            0       1         0         0           0   \n",
      "1  ...               0            0       0         1         0           0   \n",
      "2  ...               0            0       0         0         1           0   \n",
      "3  ...               0            0       0         0         0           1   \n",
      "4  ...               0            0       0         0         0           0   \n",
      "\n",
      "   Freitag  Samstag  Sonntag    VPI  \n",
      "0        0        0        0  0.052  \n",
      "1        0        0        0  0.052  \n",
      "2        0        0        0  0.052  \n",
      "3        0        0        0  0.052  \n",
      "4        1        0        0  0.052  \n",
      "\n",
      "[5 rows x 32 columns]\n",
      "(9334, 32)\n"
     ]
    }
   ],
   "source": [
    "#hinzufügen der Inflationsdaten\n",
    "#einlesen der Daten\n",
    "inflation = pd.read_csv(\"/workspaces/bakery_sales_prediction/0_DataPreparation/02_Verbraucherpreisindex/vpi_daily.csv\")\n",
    "print(inflation.head())\n",
    "print(inflation.shape)\n",
    "\n",
    "#mergen der Daten\n",
    "data = pd.merge(data, inflation, how='left', on='Datum')    \n",
    "print(data.head())\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Datum  Number_of_ships  Ship\n",
      "0  2013-07-01              0.0     0\n",
      "1  2013-07-02              0.0     0\n",
      "2  2013-07-03              0.0     0\n",
      "3  2013-07-04              0.0     0\n",
      "4  2013-07-05              1.0     1\n"
     ]
    }
   ],
   "source": [
    "#hinzufügen der Kreuzfahrschiffsdaten\n",
    "schiffe = pd.read_csv(\"/workspaces/bakery_sales_prediction/0_DataPreparation/01_Kreuzfahrtschiffe/Alle_Schiffe.csv\")\n",
    "print(schiffe.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Datum      Umsatz  Warengruppe_1  Warengruppe_2  Warengruppe_3  \\\n",
      "0  2013-07-01  148.828353              1              0              0   \n",
      "1  2013-07-02  159.793757              1              0              0   \n",
      "2  2013-07-03  111.885594              1              0              0   \n",
      "3  2013-07-04  168.864941              1              0              0   \n",
      "4  2013-07-05  171.280754              1              0              0   \n",
      "\n",
      "   Warengruppe_4  Warengruppe_5  Warengruppe_6  Temp_Very_Cold  Temp_Cold  \\\n",
      "0              0              0              0               0          0   \n",
      "1              0              0              0               0          0   \n",
      "2              0              0              0               0          0   \n",
      "3              0              0              0               0          0   \n",
      "4              0              0              0               0          0   \n",
      "\n",
      "   ...  Montag  Dienstag  Mittwoch  Donnerstag  Freitag  Samstag  Sonntag  \\\n",
      "0  ...       1         0         0           0        0        0        0   \n",
      "1  ...       0         1         0           0        0        0        0   \n",
      "2  ...       0         0         1           0        0        0        0   \n",
      "3  ...       0         0         0           1        0        0        0   \n",
      "4  ...       0         0         0           0        1        0        0   \n",
      "\n",
      "     VPI  Number_of_ships  Ship  \n",
      "0  0.052              0.0     0  \n",
      "1  0.052              0.0     0  \n",
      "2  0.052              0.0     0  \n",
      "3  0.052              0.0     0  \n",
      "4  0.052              1.0     1  \n",
      "\n",
      "[5 rows x 34 columns]\n",
      "(9334, 34)\n"
     ]
    }
   ],
   "source": [
    "#merge the data\n",
    "data = pd.merge(data, schiffe, how='left', left_on='Datum', right_on='Datum')\n",
    "\n",
    "#fill NaN with False\n",
    "data['Ship'] = data['Ship'].fillna(False)\n",
    "\n",
    "print(data.head())\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Datum  Heimspiel\n",
      "0  2013-01-01          0\n",
      "1  2013-01-02          0\n",
      "2  2013-01-03          0\n",
      "3  2013-01-04          0\n",
      "4  2013-01-05          0\n",
      "(2922, 2)\n"
     ]
    }
   ],
   "source": [
    "#importieren der heimspiel daten\n",
    "heimspiele = pd.read_csv(\"/workspaces/bakery_sales_prediction/0_DataPreparation/03_Heimspiele/Kiel_Heimspiele.csv\", sep=',')\n",
    "\n",
    "print(heimspiele.head())\n",
    "print(heimspiele.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Datum      Umsatz  Warengruppe_1  Warengruppe_2  Warengruppe_3  \\\n",
      "0  2013-07-01  148.828353              1              0              0   \n",
      "1  2013-07-02  159.793757              1              0              0   \n",
      "2  2013-07-03  111.885594              1              0              0   \n",
      "3  2013-07-04  168.864941              1              0              0   \n",
      "4  2013-07-05  171.280754              1              0              0   \n",
      "\n",
      "   Warengruppe_4  Warengruppe_5  Warengruppe_6  Temp_Very_Cold  Temp_Cold  \\\n",
      "0              0              0              0               0          0   \n",
      "1              0              0              0               0          0   \n",
      "2              0              0              0               0          0   \n",
      "3              0              0              0               0          0   \n",
      "4              0              0              0               0          0   \n",
      "\n",
      "   ...  Dienstag  Mittwoch  Donnerstag  Freitag  Samstag  Sonntag    VPI  \\\n",
      "0  ...         0         0           0        0        0        0  0.052   \n",
      "1  ...         1         0           0        0        0        0  0.052   \n",
      "2  ...         0         1           0        0        0        0  0.052   \n",
      "3  ...         0         0           1        0        0        0  0.052   \n",
      "4  ...         0         0           0        1        0        0  0.052   \n",
      "\n",
      "   Number_of_ships  Ship  Heimspiel  \n",
      "0              0.0     0          0  \n",
      "1              0.0     0          0  \n",
      "2              0.0     0          0  \n",
      "3              0.0     0          0  \n",
      "4              1.0     1          0  \n",
      "\n",
      "[5 rows x 35 columns]\n",
      "(9334, 35)\n"
     ]
    }
   ],
   "source": [
    "#mergen der heimspiel Daten\n",
    "data = pd.merge(data, heimspiele, how='left', left_on='Datum', right_on='Datum')\n",
    "print(data.head())\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Datum  Feiertag\n",
      "0  2013-07-01         0\n",
      "1  2013-07-02         0\n",
      "2  2013-07-03         0\n",
      "3  2013-07-04         0\n",
      "4  2013-07-05         0\n",
      "(2375, 2)\n"
     ]
    }
   ],
   "source": [
    "#Feiertagsdaten hochladen\n",
    "feiertage = pd.read_csv(\"/workspaces/bakery_sales_prediction/0_DataPreparation/04_Feiertage/feiertage_final.csv\", sep=';')\n",
    "print(feiertage.head())\n",
    "print(feiertage.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Datum      Umsatz  Warengruppe_1  Warengruppe_2  Warengruppe_3  \\\n",
      "0  2013-07-01  148.828353              1              0              0   \n",
      "1  2013-07-02  159.793757              1              0              0   \n",
      "2  2013-07-03  111.885594              1              0              0   \n",
      "3  2013-07-04  168.864941              1              0              0   \n",
      "4  2013-07-05  171.280754              1              0              0   \n",
      "\n",
      "   Warengruppe_4  Warengruppe_5  Warengruppe_6  Temp_Very_Cold  Temp_Cold  \\\n",
      "0              0              0              0               0          0   \n",
      "1              0              0              0               0          0   \n",
      "2              0              0              0               0          0   \n",
      "3              0              0              0               0          0   \n",
      "4              0              0              0               0          0   \n",
      "\n",
      "   ...  Mittwoch  Donnerstag  Freitag  Samstag  Sonntag    VPI  \\\n",
      "0  ...         0           0        0        0        0  0.052   \n",
      "1  ...         0           0        0        0        0  0.052   \n",
      "2  ...         1           0        0        0        0  0.052   \n",
      "3  ...         0           1        0        0        0  0.052   \n",
      "4  ...         0           0        1        0        0  0.052   \n",
      "\n",
      "   Number_of_ships  Ship  Heimspiel  Feiertag  \n",
      "0              0.0     0          0         0  \n",
      "1              0.0     0          0         0  \n",
      "2              0.0     0          0         0  \n",
      "3              0.0     0          0         0  \n",
      "4              1.0     1          0         0  \n",
      "\n",
      "[5 rows x 36 columns]\n",
      "(9334, 36)\n"
     ]
    }
   ],
   "source": [
    "#und mergen der feiertagsdaten\n",
    "data = pd.merge(data, feiertage, how='left', left_on='Datum', right_on='Datum')\n",
    "print(data.head())\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Datum  is_holiday\n",
      "0  2013-01-01           0\n",
      "1  2013-01-02           0\n",
      "2  2013-01-03           0\n",
      "3  2013-01-04           0\n",
      "4  2013-01-05           0\n",
      "        Datum      Umsatz  Warengruppe_1  Warengruppe_2  Warengruppe_3  \\\n",
      "0  2013-07-01  148.828353              1              0              0   \n",
      "1  2013-07-02  159.793757              1              0              0   \n",
      "2  2013-07-03  111.885594              1              0              0   \n",
      "3  2013-07-04  168.864941              1              0              0   \n",
      "4  2013-07-05  171.280754              1              0              0   \n",
      "\n",
      "   Warengruppe_4  Warengruppe_5  Warengruppe_6  Temp_Very_Cold  Temp_Cold  \\\n",
      "0              0              0              0               0          0   \n",
      "1              0              0              0               0          0   \n",
      "2              0              0              0               0          0   \n",
      "3              0              0              0               0          0   \n",
      "4              0              0              0               0          0   \n",
      "\n",
      "   ...  Donnerstag  Freitag  Samstag  Sonntag    VPI  Number_of_ships  Ship  \\\n",
      "0  ...           0        0        0        0  0.052              0.0     0   \n",
      "1  ...           0        0        0        0  0.052              0.0     0   \n",
      "2  ...           0        0        0        0  0.052              0.0     0   \n",
      "3  ...           1        0        0        0  0.052              0.0     0   \n",
      "4  ...           0        1        0        0  0.052              1.0     1   \n",
      "\n",
      "   Heimspiel  Feiertag  is_holiday  \n",
      "0          0         0           1  \n",
      "1          0         0           1  \n",
      "2          0         0           1  \n",
      "3          0         0           1  \n",
      "4          0         0           1  \n",
      "\n",
      "[5 rows x 37 columns]\n"
     ]
    }
   ],
   "source": [
    "#hinzufügen der Feriendaten\n",
    "ferien = pd.read_csv(\"/workspaces/bakery_sales_prediction/0_DataPreparation/06_Ferientage/06_ferientage.csv\")\n",
    "print(ferien.head())\n",
    "\n",
    "#und mergen der Feriendaten\n",
    "data = pd.merge(data, ferien, how='left', left_on='Datum', right_on='Datum')\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Datum  Weihnachtsmarkt  Markt\n",
      "0  2013-01-01                0      0\n",
      "1  2013-01-02                0      0\n",
      "2  2013-01-03                0      0\n",
      "3  2013-01-04                0      0\n",
      "4  2013-01-05                0      0\n",
      "        Datum      Umsatz  Warengruppe_1  Warengruppe_2  Warengruppe_3  \\\n",
      "0  2013-07-01  148.828353              1              0              0   \n",
      "1  2013-07-02  159.793757              1              0              0   \n",
      "2  2013-07-03  111.885594              1              0              0   \n",
      "3  2013-07-04  168.864941              1              0              0   \n",
      "4  2013-07-05  171.280754              1              0              0   \n",
      "\n",
      "   Warengruppe_4  Warengruppe_5  Warengruppe_6  Temp_Very_Cold  Temp_Cold  \\\n",
      "0              0              0              0               0          0   \n",
      "1              0              0              0               0          0   \n",
      "2              0              0              0               0          0   \n",
      "3              0              0              0               0          0   \n",
      "4              0              0              0               0          0   \n",
      "\n",
      "   ...  Samstag  Sonntag    VPI  Number_of_ships  Ship  Heimspiel  Feiertag  \\\n",
      "0  ...        0        0  0.052              0.0     0          0         0   \n",
      "1  ...        0        0  0.052              0.0     0          0         0   \n",
      "2  ...        0        0  0.052              0.0     0          0         0   \n",
      "3  ...        0        0  0.052              0.0     0          0         0   \n",
      "4  ...        0        0  0.052              1.0     1          0         0   \n",
      "\n",
      "   is_holiday  Weihnachtsmarkt  Markt  \n",
      "0           1                0      0  \n",
      "1           1                0      0  \n",
      "2           1                0      0  \n",
      "3           1                0      0  \n",
      "4           1                0      0  \n",
      "\n",
      "[5 rows x 39 columns]\n"
     ]
    }
   ],
   "source": [
    "#hinzufügen der Märkte\n",
    "marktdaten = pd.read_csv(\"/workspaces/bakery_sales_prediction/0_DataPreparation/08_Maerkte/maekte_final.csv\")\n",
    "print(marktdaten.head())\n",
    "\n",
    "#und mergen der Marktdaten\n",
    "data = pd.merge(data, marktdaten, how='left', left_on='Datum', right_on='Datum')\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Datum  Ostertag\n",
      "0  2013-07-01         0\n",
      "1  2013-07-02         0\n",
      "2  2013-07-03         0\n",
      "3  2013-07-04         0\n",
      "4  2013-07-05         0\n",
      "        Datum      Umsatz  Warengruppe_1  Warengruppe_2  Warengruppe_3  \\\n",
      "0  2013-07-01  148.828353              1              0              0   \n",
      "1  2013-07-02  159.793757              1              0              0   \n",
      "2  2013-07-03  111.885594              1              0              0   \n",
      "3  2013-07-04  168.864941              1              0              0   \n",
      "4  2013-07-05  171.280754              1              0              0   \n",
      "\n",
      "   Warengruppe_4  Warengruppe_5  Warengruppe_6  Temp_Very_Cold  Temp_Cold  \\\n",
      "0              0              0              0               0          0   \n",
      "1              0              0              0               0          0   \n",
      "2              0              0              0               0          0   \n",
      "3              0              0              0               0          0   \n",
      "4              0              0              0               0          0   \n",
      "\n",
      "   ...  Sonntag    VPI  Number_of_ships  Ship  Heimspiel  Feiertag  \\\n",
      "0  ...        0  0.052              0.0     0          0         0   \n",
      "1  ...        0  0.052              0.0     0          0         0   \n",
      "2  ...        0  0.052              0.0     0          0         0   \n",
      "3  ...        0  0.052              0.0     0          0         0   \n",
      "4  ...        0  0.052              1.0     1          0         0   \n",
      "\n",
      "   is_holiday  Weihnachtsmarkt  Markt  Ostertag  \n",
      "0           1                0      0         0  \n",
      "1           1                0      0         0  \n",
      "2           1                0      0         0  \n",
      "3           1                0      0         0  \n",
      "4           1                0      0         0  \n",
      "\n",
      "[5 rows x 40 columns]\n"
     ]
    }
   ],
   "source": [
    "#hinzufügen der Ostertage (Ostersamstag und Gründonnerstag)\n",
    "Ostertag = pd.read_csv(\"/workspaces/bakery_sales_prediction/0_DataPreparation/10_Ostertag/Ostertage.csv\")\n",
    "print(Ostertag.head())\n",
    "\n",
    "#und mergen der Ostertage\n",
    "data = pd.merge(data, Ostertag, how='left', left_on='Datum', right_on='Datum')\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Datum      Umsatz  Warengruppe_1  Warengruppe_2  Warengruppe_3  \\\n",
      "0  2013-07-01  148.828353              1              0              0   \n",
      "1  2013-07-02  159.793757              1              0              0   \n",
      "2  2013-07-03  111.885594              1              0              0   \n",
      "3  2013-07-04  168.864941              1              0              0   \n",
      "4  2013-07-05  171.280754              1              0              0   \n",
      "\n",
      "   Warengruppe_4  Warengruppe_5  Warengruppe_6  Temp_Very_Cold  Temp_Cold  \\\n",
      "0              0              0              0               0          0   \n",
      "1              0              0              0               0          0   \n",
      "2              0              0              0               0          0   \n",
      "3              0              0              0               0          0   \n",
      "4              0              0              0               0          0   \n",
      "\n",
      "   ...    VPI  Number_of_ships  Ship  Heimspiel  Feiertag  is_holiday  \\\n",
      "0  ...  0.052              0.0     0          0         0           1   \n",
      "1  ...  0.052              0.0     0          0         0           1   \n",
      "2  ...  0.052              0.0     0          0         0           1   \n",
      "3  ...  0.052              0.0     0          0         0           1   \n",
      "4  ...  0.052              1.0     1          0         0           1   \n",
      "\n",
      "   Weihnachtsmarkt  Markt  Ostertag  Silvester  \n",
      "0                0      0         0          0  \n",
      "1                0      0         0          0  \n",
      "2                0      0         0          0  \n",
      "3                0      0         0          0  \n",
      "4                0      0         0          0  \n",
      "\n",
      "[5 rows x 41 columns]\n"
     ]
    }
   ],
   "source": [
    "#hinzufügen der Silvestertage\n",
    "Silvester = pd.read_csv(\"/workspaces/bakery_sales_prediction/0_DataPreparation/11_Silvester/Silvester.csv\")\n",
    "\n",
    "#und mergen der Silvestertae\n",
    "data = pd.merge(data, Silvester, how='left', left_on='Datum', right_on='Datum')\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Datum  wetter_sehr_schön  wetter_sehr_schlecht\n",
      "0  2013-07-01                  0                     0\n",
      "1  2013-07-02                  0                     0\n",
      "2  2013-07-03                  0                     1\n",
      "3  2013-07-04                  0                     0\n",
      "4  2013-07-05                  0                     0\n",
      "        Datum      Umsatz  Warengruppe_1  Warengruppe_2  Warengruppe_3  \\\n",
      "0  2013-07-01  148.828353              1              0              0   \n",
      "1  2013-07-02  159.793757              1              0              0   \n",
      "2  2013-07-03  111.885594              1              0              0   \n",
      "3  2013-07-04  168.864941              1              0              0   \n",
      "4  2013-07-05  171.280754              1              0              0   \n",
      "\n",
      "   Warengruppe_4  Warengruppe_5  Warengruppe_6  Temp_Very_Cold  Temp_Cold  \\\n",
      "0              0              0              0               0          0   \n",
      "1              0              0              0               0          0   \n",
      "2              0              0              0               0          0   \n",
      "3              0              0              0               0          0   \n",
      "4              0              0              0               0          0   \n",
      "\n",
      "   ...  Ship  Heimspiel  Feiertag  is_holiday  Weihnachtsmarkt  Markt  \\\n",
      "0  ...     0          0         0           1                0      0   \n",
      "1  ...     0          0         0           1                0      0   \n",
      "2  ...     0          0         0           1                0      0   \n",
      "3  ...     0          0         0           1                0      0   \n",
      "4  ...     1          0         0           1                0      0   \n",
      "\n",
      "   Ostertag  Silvester  wetter_sehr_schön  wetter_sehr_schlecht  \n",
      "0         0          0                  0                     0  \n",
      "1         0          0                  0                     0  \n",
      "2         0          0                  0                     1  \n",
      "3         0          0                  0                     0  \n",
      "4         0          0                  0                     0  \n",
      "\n",
      "[5 rows x 43 columns]\n"
     ]
    }
   ],
   "source": [
    "#hinzufügen der Ersatz-Wettercodes\n",
    "wettercode_ersatz = pd.read_csv(\"/workspaces/bakery_sales_prediction/0_DataPreparation/12_Wettercode_Ersatz/wettercode_ersatz.csv\")\n",
    "#Umbennen der Spalte \"date\" in \"Datum\"\n",
    "wettercode_ersatz = wettercode_ersatz.rename(columns={\"date\": \"Datum\"})\n",
    "print(wettercode_ersatz.head())\n",
    "\n",
    "#und mergen der Ersatz-Wettercodes\n",
    "data = pd.merge(data, wettercode_ersatz, how='left', left_on='Datum', right_on='Datum')\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Datum      Umsatz  Warengruppe_1  Warengruppe_2  Warengruppe_3  \\\n",
      "0  2013-07-01  148.828353              1              0              0   \n",
      "1  2013-07-02  159.793757              1              0              0   \n",
      "2  2013-07-03  111.885594              1              0              0   \n",
      "3  2013-07-04  168.864941              1              0              0   \n",
      "4  2013-07-05  171.280754              1              0              0   \n",
      "\n",
      "   Warengruppe_4  Warengruppe_5  Warengruppe_6  Temp_Very_Cold  Temp_Cold  \\\n",
      "0              0              0              0               0          0   \n",
      "1              0              0              0               0          0   \n",
      "2              0              0              0               0          0   \n",
      "3              0              0              0               0          0   \n",
      "4              0              0              0               0          0   \n",
      "\n",
      "   ...  Weihnachtsmarkt  Markt  Ostertag  Silvester  wetter_sehr_schön  \\\n",
      "0  ...                0      0         0          0                  0   \n",
      "1  ...                0      0         0          0                  0   \n",
      "2  ...                0      0         0          0                  0   \n",
      "3  ...                0      0         0          0                  0   \n",
      "4  ...                0      0         0          0                  0   \n",
      "\n",
      "   wetter_sehr_schlecht  Frühling  Sommer  Herbst  Winter  \n",
      "0                     0         0       1       0       0  \n",
      "1                     0         0       1       0       0  \n",
      "2                     1         0       1       0       0  \n",
      "3                     0         0       1       0       0  \n",
      "4                     0         0       1       0       0  \n",
      "\n",
      "[5 rows x 47 columns]\n"
     ]
    }
   ],
   "source": [
    "#hinzufügen der Jahreszeiten\n",
    "Jahreszeiten = pd.read_csv(\"/workspaces/bakery_sales_prediction/0_DataPreparation/13_Jahreszeiten/Jahreszeiten.csv\")\n",
    "\n",
    "#und mergen der Jahreszeiten\n",
    "data = pd.merge(data, Jahreszeiten, how='left', left_on='Datum', right_on='Datum')\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Datum      Umsatz  Warengruppe_1  Warengruppe_2  Warengruppe_3  \\\n",
      "0  2013-07-01  148.828353              1              0              0   \n",
      "1  2013-07-02  159.793757              1              0              0   \n",
      "2  2013-07-03  111.885594              1              0              0   \n",
      "3  2013-07-04  168.864941              1              0              0   \n",
      "4  2013-07-05  171.280754              1              0              0   \n",
      "\n",
      "   Warengruppe_4  Warengruppe_5  Warengruppe_6  Temp_Very_Cold  Temp_Cold  \\\n",
      "0              0              0              0               0          0   \n",
      "1              0              0              0               0          0   \n",
      "2              0              0              0               0          0   \n",
      "3              0              0              0               0          0   \n",
      "4              0              0              0               0          0   \n",
      "\n",
      "   ...  Markt  Ostertag  Silvester  wetter_sehr_schön  wetter_sehr_schlecht  \\\n",
      "0  ...      0         0          0                  0                     0   \n",
      "1  ...      0         0          0                  0                     0   \n",
      "2  ...      0         0          0                  0                     1   \n",
      "3  ...      0         0          0                  0                     0   \n",
      "4  ...      0         0          0                  0                     0   \n",
      "\n",
      "   Frühling  Sommer  Herbst  Winter  Werktag  \n",
      "0         0       1       0       0        1  \n",
      "1         0       1       0       0        1  \n",
      "2         0       1       0       0        1  \n",
      "3         0       1       0       0        1  \n",
      "4         0       1       0       0        1  \n",
      "\n",
      "[5 rows x 48 columns]\n"
     ]
    }
   ],
   "source": [
    "#hinzufügen der Werktage\n",
    "Werktage = pd.read_csv(\"/workspaces/bakery_sales_prediction/0_DataPreparation/14_Werktag/Werktag.csv\")\n",
    "\n",
    "#und mergen der Werktage\n",
    "data = pd.merge(data, Werktage, how='left', left_on='Datum', right_on='Datum')\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Datum      Umsatz  Warengruppe_1  Warengruppe_2  Warengruppe_3  \\\n",
      "0  2013-07-01  148.828353              1              0              0   \n",
      "1  2013-07-02  159.793757              1              0              0   \n",
      "2  2013-07-03  111.885594              1              0              0   \n",
      "3  2013-07-04  168.864941              1              0              0   \n",
      "4  2013-07-05  171.280754              1              0              0   \n",
      "\n",
      "   Warengruppe_4  Warengruppe_5  Warengruppe_6  Temp_Very_Cold  Temp_Cold  \\\n",
      "0              0              0              0               0          0   \n",
      "1              0              0              0               0          0   \n",
      "2              0              0              0               0          0   \n",
      "3              0              0              0               0          0   \n",
      "4              0              0              0               0          0   \n",
      "\n",
      "   ...  Monat_4  Monat_5  Monat_6  Monat_7  Monat_8  Monat_9  Monat_10  \\\n",
      "0  ...        0        0        0        1        0        0         0   \n",
      "1  ...        0        0        0        1        0        0         0   \n",
      "2  ...        0        0        0        1        0        0         0   \n",
      "3  ...        0        0        0        1        0        0         0   \n",
      "4  ...        0        0        0        1        0        0         0   \n",
      "\n",
      "   Monat_11  Monat_12  zwischen_den_jahren  \n",
      "0         0         0                    0  \n",
      "1         0         0                    0  \n",
      "2         0         0                    0  \n",
      "3         0         0                    0  \n",
      "4         0         0                    0  \n",
      "\n",
      "[5 rows x 61 columns]\n"
     ]
    }
   ],
   "source": [
    "#hinzufügen der Monate und \"zwischen_den_jahren\"\n",
    "Monate = pd.read_csv(\"/workspaces/bakery_sales_prediction/0_DataPreparation/15_Monat_und_zwischen_Jahren/Monate.csv\")\n",
    "\n",
    "#und mergen\n",
    "data = pd.merge(data, Monate, how='left', left_on='Datum', right_on='Datum')\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Datum      Umsatz  Warengruppe_1  Warengruppe_2  Warengruppe_3  \\\n",
      "0  2013-07-01  148.828353              1              0              0   \n",
      "1  2013-07-02  159.793757              1              0              0   \n",
      "2  2013-07-03  111.885594              1              0              0   \n",
      "3  2013-07-04  168.864941              1              0              0   \n",
      "4  2013-07-05  171.280754              1              0              0   \n",
      "\n",
      "   Warengruppe_4  Warengruppe_5  Warengruppe_6  Temp_Very_Cold  Temp_Cold  \\\n",
      "0              0              0              0               0          0   \n",
      "1              0              0              0               0          0   \n",
      "2              0              0              0               0          0   \n",
      "3              0              0              0               0          0   \n",
      "4              0              0              0               0          0   \n",
      "\n",
      "   ...  Monat_7  Monat_8  Monat_9  Monat_10  Monat_11  Monat_12  \\\n",
      "0  ...        1        0        0         0         0         0   \n",
      "1  ...        1        0        0         0         0         0   \n",
      "2  ...        1        0        0         0         0         0   \n",
      "3  ...        1        0        0         0         0         0   \n",
      "4  ...        1        0        0         0         0         0   \n",
      "\n",
      "   zwischen_den_jahren  Number_of_ships_scaled  Cloud_ok  Wind_ok  \n",
      "0                    0                     0.0         0        1  \n",
      "1                    0                     0.0         1        1  \n",
      "2                    0                     0.0         0        1  \n",
      "3                    0                     0.0         0        1  \n",
      "4                    0                     0.2         1        1  \n",
      "\n",
      "[5 rows x 64 columns]\n"
     ]
    }
   ],
   "source": [
    "# Number of ships skalieren und Wetter zusammenfassen\n",
    "\n",
    "#Number of Ships skalieren\n",
    "# Min-Max-Skalierung manuell berechnen\n",
    "data['Number_of_ships_scaled'] = (\n",
    "    data['Number_of_ships'] - data['Number_of_ships'].min()\n",
    ") / (data['Number_of_ships'].max() - data['Number_of_ships'].min())\n",
    "\n",
    "#Bewölkung zsmfassen\n",
    "data['Cloud_ok'] = (\n",
    "    data[['Cloud_Clear', 'Cloud_Partly_Cloudy']].sum(axis=1) > 0)\n",
    "# In 0/1 umwandeln\n",
    "data['Cloud_ok'] = data['Cloud_ok'].astype(int)\n",
    "\n",
    "#wind zsmfassen\n",
    "data['Wind_ok'] = (\n",
    "    data[['Wind_Light', 'Wind_Moderate']].sum(axis=1) > 0)\n",
    "# In 0/1 umwandeln\n",
    "data['Wind_ok'] = data['Wind_ok'].astype(int)\n",
    "\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Damit sind alle Daten gemerged und der df kann als csv zur weiteren Bearbeitung exportiert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exportieren der Daten als csv\n",
    "data.to_csv(\"data_long.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7493, 64)\n",
      "(1841, 64)\n"
     ]
    }
   ],
   "source": [
    "#teilern der Daten in einen Trainingsdatensatz und einen Validierungsdatensatz\n",
    "#trinaingsdaten sollen vom 01.07.2013 bis 31.07.20217 gehen\n",
    "#validierungsdaten sollen vom 01.08.2017 bis 31.07.2018 gehen\n",
    "train = data[data['Datum'] < '2017-08-01']\n",
    "valid = data[data['Datum'] >= '2017-08-01']\n",
    "\n",
    "print(train.shape)\n",
    "print(valid.shape)\n",
    "\n",
    "#exportieren der Daten als csv\n",
    "train.to_csv(\"Trainingsdaten_long.csv\", index=False)\n",
    "valid.to_csv(\"Validierungsdaten_long.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Die Daten des Vortages und nachfolgenden Tages werden jedem Tag hinzugefügt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_23187/199099272.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result[f'{col}_next'] = df_next[col]\n",
      "/tmp/ipykernel_23187/199099272.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result[f'{col}_current'] = df[col]\n",
      "/tmp/ipykernel_23187/199099272.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result[f'{col}_prev'] = df_prev[col]\n",
      "/tmp/ipykernel_23187/199099272.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result[f'{col}_next'] = df_next[col]\n",
      "/tmp/ipykernel_23187/199099272.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result[f'{col}_current'] = df[col]\n",
      "/tmp/ipykernel_23187/199099272.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result[f'{col}_prev'] = df_prev[col]\n",
      "/tmp/ipykernel_23187/199099272.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result[f'{col}_next'] = df_next[col]\n",
      "/tmp/ipykernel_23187/199099272.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result[f'{col}_current'] = df[col]\n",
      "/tmp/ipykernel_23187/199099272.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result[f'{col}_prev'] = df_prev[col]\n",
      "/tmp/ipykernel_23187/199099272.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result[f'{col}_next'] = df_next[col]\n",
      "/tmp/ipykernel_23187/199099272.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result[f'{col}_current'] = df[col]\n",
      "/tmp/ipykernel_23187/199099272.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result[f'{col}_prev'] = df_prev[col]\n",
      "/tmp/ipykernel_23187/199099272.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result[f'{col}_next'] = df_next[col]\n",
      "/tmp/ipykernel_23187/199099272.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result[f'{col}_current'] = df[col]\n",
      "/tmp/ipykernel_23187/199099272.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result[f'{col}_prev'] = df_prev[col]\n",
      "/tmp/ipykernel_23187/199099272.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result[f'{col}_next'] = df_next[col]\n",
      "/tmp/ipykernel_23187/199099272.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result[f'{col}_current'] = df[col]\n",
      "/tmp/ipykernel_23187/199099272.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result[f'{col}_prev'] = df_prev[col]\n",
      "/tmp/ipykernel_23187/199099272.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result[f'{col}_next'] = df_next[col]\n",
      "/tmp/ipykernel_23187/199099272.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result[f'{col}_current'] = df[col]\n",
      "/tmp/ipykernel_23187/199099272.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result[f'{col}_prev'] = df_prev[col]\n",
      "/tmp/ipykernel_23187/199099272.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result[f'{col}_next'] = df_next[col]\n",
      "/tmp/ipykernel_23187/199099272.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result[f'{col}_current'] = df[col]\n",
      "/tmp/ipykernel_23187/199099272.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result[f'{col}_prev'] = df_prev[col]\n",
      "/tmp/ipykernel_23187/199099272.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result[f'{col}_next'] = df_next[col]\n",
      "/tmp/ipykernel_23187/199099272.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result[f'{col}_current'] = df[col]\n",
      "/tmp/ipykernel_23187/199099272.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result[f'{col}_prev'] = df_prev[col]\n",
      "/tmp/ipykernel_23187/199099272.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result[f'{col}_next'] = df_next[col]\n",
      "/tmp/ipykernel_23187/199099272.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result[f'{col}_current'] = df[col]\n",
      "/tmp/ipykernel_23187/199099272.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result[f'{col}_prev'] = df_prev[col]\n",
      "/tmp/ipykernel_23187/199099272.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result[f'{col}_next'] = df_next[col]\n",
      "/tmp/ipykernel_23187/199099272.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result[f'{col}_current'] = df[col]\n",
      "/tmp/ipykernel_23187/199099272.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result[f'{col}_prev'] = df_prev[col]\n",
      "/tmp/ipykernel_23187/199099272.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result[f'{col}_next'] = df_next[col]\n",
      "/tmp/ipykernel_23187/199099272.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result[f'{col}_current'] = df[col]\n",
      "/tmp/ipykernel_23187/199099272.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result[f'{col}_prev'] = df_prev[col]\n",
      "/tmp/ipykernel_23187/199099272.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result[f'{col}_next'] = df_next[col]\n",
      "/tmp/ipykernel_23187/199099272.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result[f'{col}_current'] = df[col]\n",
      "/tmp/ipykernel_23187/199099272.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result[f'{col}_prev'] = df_prev[col]\n",
      "/tmp/ipykernel_23187/199099272.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result[f'{col}_next'] = df_next[col]\n",
      "/tmp/ipykernel_23187/199099272.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result[f'{col}_current'] = df[col]\n",
      "/tmp/ipykernel_23187/199099272.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result[f'{col}_prev'] = df_prev[col]\n",
      "/tmp/ipykernel_23187/199099272.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result[f'{col}_next'] = df_next[col]\n",
      "/tmp/ipykernel_23187/199099272.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result[f'{col}_current'] = df[col]\n",
      "/tmp/ipykernel_23187/199099272.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result[f'{col}_prev'] = df_prev[col]\n",
      "/tmp/ipykernel_23187/199099272.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result[f'{col}_next'] = df_next[col]\n",
      "/tmp/ipykernel_23187/199099272.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result[f'{col}_current'] = df[col]\n",
      "/tmp/ipykernel_23187/199099272.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result[f'{col}_prev'] = df_prev[col]\n",
      "/tmp/ipykernel_23187/199099272.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result[f'{col}_next'] = df_next[col]\n",
      "/tmp/ipykernel_23187/199099272.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result[f'{col}_current'] = df[col]\n",
      "/tmp/ipykernel_23187/199099272.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result[f'{col}_prev'] = df_prev[col]\n",
      "/tmp/ipykernel_23187/199099272.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result[f'{col}_next'] = df_next[col]\n",
      "/tmp/ipykernel_23187/199099272.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result[f'{col}_current'] = df[col]\n",
      "/tmp/ipykernel_23187/199099272.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result[f'{col}_prev'] = df_prev[col]\n",
      "/tmp/ipykernel_23187/199099272.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result[f'{col}_next'] = df_next[col]\n",
      "/tmp/ipykernel_23187/199099272.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result[f'{col}_current'] = df[col]\n",
      "/tmp/ipykernel_23187/199099272.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result[f'{col}_prev'] = df_prev[col]\n",
      "/tmp/ipykernel_23187/199099272.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result[f'{col}_next'] = df_next[col]\n",
      "/tmp/ipykernel_23187/199099272.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result[f'{col}_current'] = df[col]\n",
      "/tmp/ipykernel_23187/199099272.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result[f'{col}_prev'] = df_prev[col]\n",
      "/tmp/ipykernel_23187/199099272.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result[f'{col}_next'] = df_next[col]\n",
      "/tmp/ipykernel_23187/199099272.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result[f'{col}_current'] = df[col]\n",
      "/tmp/ipykernel_23187/199099272.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result[f'{col}_prev'] = df_prev[col]\n",
      "/tmp/ipykernel_23187/199099272.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result[f'{col}_next'] = df_next[col]\n",
      "/tmp/ipykernel_23187/199099272.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result[f'{col}_current'] = df[col]\n",
      "/tmp/ipykernel_23187/199099272.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result[f'{col}_prev'] = df_prev[col]\n",
      "/tmp/ipykernel_23187/199099272.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result[f'{col}_next'] = df_next[col]\n",
      "/tmp/ipykernel_23187/199099272.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result[f'{col}_current'] = df[col]\n",
      "/tmp/ipykernel_23187/199099272.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result[f'{col}_prev'] = df_prev[col]\n",
      "/tmp/ipykernel_23187/199099272.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result[f'{col}_next'] = df_next[col]\n",
      "/tmp/ipykernel_23187/199099272.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result[f'{col}_current'] = df[col]\n",
      "/tmp/ipykernel_23187/199099272.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result[f'{col}_prev'] = df_prev[col]\n",
      "/tmp/ipykernel_23187/199099272.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result[f'{col}_next'] = df_next[col]\n",
      "/tmp/ipykernel_23187/199099272.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result[f'{col}_current'] = df[col]\n",
      "/tmp/ipykernel_23187/199099272.py:32: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result[f'{col}_prev'] = df_prev[col]\n",
      "/tmp/ipykernel_23187/199099272.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result[f'{col}_next'] = df_next[col]\n"
     ]
    }
   ],
   "source": [
    "df = data\n",
    "\n",
    "# Create copies of the dataframe shifted by 1 day forward and backward\n",
    "df_prev = df.shift(1)\n",
    "df_next = df.shift(-1)\n",
    "\n",
    "# For the first row, use current day's values instead of previous\n",
    "df_prev.iloc[0] = df.iloc[0]\n",
    "\n",
    "# For the last row, use current day's values instead of next\n",
    "df_next.iloc[-1] = df.iloc[-1]\n",
    "\n",
    "# Create a new dataframe for the result\n",
    "result = pd.DataFrame()\n",
    "\n",
    "# Keep original Datum and Umsatz\n",
    "result['Datum'] = df['Datum']\n",
    "result['Umsatz'] = df['Umsatz']\n",
    "result['Warengruppe_1'] = df['Warengruppe_1']\n",
    "result['Warengruppe_2'] = df['Warengruppe_2']\n",
    "result['Warengruppe_3'] = df['Warengruppe_3']\n",
    "result['Warengruppe_4'] = df['Warengruppe_4']\n",
    "result['Warengruppe_5'] = df['Warengruppe_5']\n",
    "result['Warengruppe_6'] = df['Warengruppe_6']\n",
    "\n",
    "# For all other columns, create three versions: original, previous, and next\n",
    "for col in df.columns:\n",
    "    if col not in ['Datum', 'Umsatz', 'Warengruppe_1', 'Warengruppe_2', 'Warengruppe_3', 'Warengruppe_4', 'Warengruppe_5', 'Warengruppe_6']:\n",
    "        # Original value\n",
    "        result[f'{col}_current'] = df[col]\n",
    "        # Previous day's value\n",
    "        result[f'{col}_prev'] = df_prev[col]\n",
    "        # Next day's value\n",
    "        result[f'{col}_next'] = df_next[col]\n",
    "\n",
    "# Save to new CSV file\n",
    "result.to_csv(\"/workspaces/bakery_sales_prediction/0_DataPreparation/00_data/data_with_lag/data_with_lag.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7493, 64)\n",
      "(1841, 64)\n"
     ]
    }
   ],
   "source": [
    "#teilern der Daten in einen Trainingsdatensatz und einen Validierungsdatensatz\n",
    "#trainingsdaten sollen vom 01.07.2013 bis 31.07.20217 gehen\n",
    "#validierungsdaten sollen vom 01.08.2017 bis 31.07.2018 gehen\n",
    "train_with_lag = result[result['Datum'] < '2017-08-01']\n",
    "valid_with_lag = result[result['Datum'] >= '2017-08-01']\n",
    "\n",
    "print(train.shape)\n",
    "print(valid.shape)\n",
    "\n",
    "#exportieren der Daten als csv\n",
    "train_with_lag.to_csv(\"/workspaces/bakery_sales_prediction/0_DataPreparation/00_data/data_with_lag/Trainingsdaten_with_lag.csv\", index=False)\n",
    "valid_with_lag.to_csv(\"/workspaces/bakery_sales_prediction/0_DataPreparation/00_data/data_with_lag/Validierungsdaten_with_lag.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
