{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-07 20:42:45.325346: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-07 20:42:45.352648: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1736278965.378503 1912338 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1736278965.386146 1912338 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-07 20:42:45.433905: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Training features dimensions: (1462, 30)\n",
      "Loaded Validation features dimensions: (357, 30)\n",
      "Loaded Test features dimensions: (355, 30)\n",
      "\n",
      "Loaded Training labels dimensions: (1462, 1)\n",
      "Loaded Validation labels dimensions: (357, 1)\n",
      "\n",
      "First few rows of loaded training features:\n",
      "   Temp_Very_Cold_current  Temp_Cold_current  Temp_Mild_current  \\\n",
      "0                       0                  0                  1   \n",
      "1                       0                  0                  1   \n",
      "2                       0                  0                  0   \n",
      "3                       0                  0                  1   \n",
      "4                       0                  0                  1   \n",
      "\n",
      "   Temp_Warm_current  Temp_Hot_current  Cloud_ok_current  \\\n",
      "0                  0                 0                 0   \n",
      "1                  0                 0                 1   \n",
      "2                  1                 0                 0   \n",
      "3                  0                 0                 0   \n",
      "4                  0                 0                 1   \n",
      "\n",
      "   Cloud_Cloudy_current  KielerWoche_current  Montag_current  \\\n",
      "0                     1                    0               1   \n",
      "1                     0                    0               0   \n",
      "2                     1                    0               0   \n",
      "3                     1                    0               0   \n",
      "4                     0                    0               0   \n",
      "\n",
      "   Dienstag_current  ...  Silvester_current  Werktag_current  Sommer_current  \\\n",
      "0                 0  ...                  0                1               1   \n",
      "1                 1  ...                  0                1               1   \n",
      "2                 0  ...                  0                1               1   \n",
      "3                 0  ...                  0                1               1   \n",
      "4                 0  ...                  0                1               1   \n",
      "\n",
      "   Winter_current  wetter_sehr_schlecht_current  wetter_sehr_schön_current  \\\n",
      "0               0                             0                          0   \n",
      "1               0                             0                          0   \n",
      "2               0                             1                          0   \n",
      "3               0                             0                          0   \n",
      "4               0                             0                          0   \n",
      "\n",
      "   Ostertag_next  Feiertag_next  Weihnachtsmarkt_next  Heimspiel_prev  \n",
      "0            0.0            0.0                   0.0             0.0  \n",
      "1            0.0            0.0                   0.0             0.0  \n",
      "2            0.0            0.0                   0.0             0.0  \n",
      "3            0.0            0.0                   0.0             0.0  \n",
      "4            0.0            0.0                   0.0             0.0  \n",
      "\n",
      "[5 rows x 30 columns]\n",
      "\n",
      "First few rows of loaded training labels:\n",
      "       Umsatz\n",
      "0  148.828353\n",
      "1  159.793757\n",
      "2  111.885594\n",
      "3  168.864941\n",
      "4  171.280754\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary libraries\n",
    "import pandas as pd\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras.layers import Dense, Dropout, InputLayer\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import AdamW\n",
    "from tensorflow.keras.callbacks import (\n",
    "    EarlyStopping, \n",
    "    ReduceLROnPlateau, \n",
    "    ModelCheckpoint, \n",
    "    TensorBoard,\n",
    "    CSVLogger\n",
    ")\n",
    "import os\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "# Import the necessary libraries\n",
    "\n",
    "\n",
    "\n",
    "# Define the file paths\n",
    "subdirectory = \"pickle_data_Brot\"\n",
    "training_features_path = f\"{subdirectory}/training_features.pkl\"\n",
    "validation_features_path = f\"{subdirectory}/validation_features.pkl\"\n",
    "test_features_path = f\"{subdirectory}/test_features.pkl\"\n",
    "training_labels_path = f\"{subdirectory}/training_labels.pkl\"\n",
    "validation_labels_path = f\"{subdirectory}/validation_labels.pkl\"\n",
    "test_labels_path = f\"{subdirectory}/test_labels.pkl\"\n",
    "\n",
    "# Read the pickle files\n",
    "training_features = pd.read_pickle(training_features_path)\n",
    "validation_features = pd.read_pickle(validation_features_path)\n",
    "test_features = pd.read_pickle(test_features_path)\n",
    "training_labels = pd.read_pickle(training_labels_path)\n",
    "validation_labels = pd.read_pickle(validation_labels_path)\n",
    "#test_labels = pd.read_pickle(test_labels_path)\n",
    "\n",
    "# Verify the loaded data by printing their shapes and a few rows\n",
    "print(\"Loaded Training features dimensions:\", training_features.shape)\n",
    "print(\"Loaded Validation features dimensions:\", validation_features.shape)\n",
    "print(\"Loaded Test features dimensions:\", test_features.shape)\n",
    "print()\n",
    "print(\"Loaded Training labels dimensions:\", training_labels.shape)\n",
    "print(\"Loaded Validation labels dimensions:\", validation_labels.shape)\n",
    "#print(\"Loaded Test labels dimensions:\", test_labels.shape)\n",
    "print()\n",
    "\n",
    "print(\"First few rows of loaded training features:\")\n",
    "print(training_features.head())\n",
    "print()\n",
    "print(\"First few rows of loaded training labels:\")\n",
    "print(training_labels.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianNeuralTuner:\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_shape,\n",
    "        max_trials=10,\n",
    "        project_name='nn_tuning',\n",
    "        log_dir='logs',\n",
    "        n_folds=3\n",
    "    ):\n",
    "        self.input_shape = input_shape\n",
    "        self.max_trials = max_trials\n",
    "        self.project_name = project_name\n",
    "        self.log_dir = os.path.join(log_dir, datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "        self.n_folds = n_folds\n",
    "        os.makedirs(self.log_dir, exist_ok=True)\n",
    "        \n",
    "    def build_model(self, hp):\n",
    "        # Use factors of 2 for more efficient GPU utilization\n",
    "        units1_choices = [32, 64, 96, 128]\n",
    "        units2_choices = [16, 32, 48, 64]\n",
    "        \n",
    "        model = Sequential([\n",
    "            InputLayer(shape=self.input_shape),\n",
    "            \n",
    "            # First dropout with tunable rate\n",
    "            Dropout(hp.Float('dropout1', 0.1, 0.3, step=0.1)),\n",
    "            \n",
    "            # First dense layer with conditional units\n",
    "            Dense(\n",
    "                units=hp.Choice('units1', units1_choices),\n",
    "                activation=hp.Choice('activation1', ['relu', 'selu']),\n",
    "                kernel_regularizer=l2(hp.Float(\n",
    "                    'l2_reg1', \n",
    "                    1e-4, \n",
    "                    1e-2, \n",
    "                    sampling='log'\n",
    "                )),\n",
    "                kernel_initializer='he_normal'\n",
    "            ),\n",
    "            \n",
    "            # Second dense layer with conditional units\n",
    "            Dense(\n",
    "                units=hp.Choice('units2', units2_choices),\n",
    "                activation=hp.Choice('activation2', ['relu', 'selu']),\n",
    "                kernel_regularizer=l2(hp.Float(\n",
    "                    'l2_reg2', \n",
    "                    1e-4, \n",
    "                    1e-2, \n",
    "                    sampling='log'\n",
    "                )),\n",
    "                kernel_initializer='he_normal'\n",
    "            ),\n",
    "            \n",
    "            Dense(1)\n",
    "        ])\n",
    "        \n",
    "        # Learning rate with warm restarts\n",
    "        base_lr = hp.Float(\n",
    "            'base_learning_rate',\n",
    "            min_value=1e-4,\n",
    "            max_value=1e-2,\n",
    "            sampling='log'\n",
    "        )\n",
    "        \n",
    "        # Optimizer with tunable parameters\n",
    "        optimizer = AdamW(\n",
    "            learning_rate=base_lr,\n",
    "            beta_1=hp.Float('beta_1', 0.9, 0.999),\n",
    "            beta_2=hp.Float('beta_2', 0.9, 0.999),\n",
    "            weight_decay=hp.Float('weight_decay', 1e-4, 1e-2, sampling='log')\n",
    "        )\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss='mse',\n",
    "            metrics=['mape']\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def get_callbacks(self, checkpoint_path, fold=None):\n",
    "        fold_suffix = f'_fold_{fold}' if fold is not None else ''\n",
    "        return [\n",
    "            EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=5,\n",
    "                restore_best_weights=True,\n",
    "                verbose=1\n",
    "            ),\n",
    "            ReduceLROnPlateau(\n",
    "                monitor='val_loss',\n",
    "                factor=0.5,\n",
    "                patience=3,\n",
    "                min_lr=1e-6,\n",
    "                verbose=1\n",
    "            ),\n",
    "            ModelCheckpoint(\n",
    "                filepath=checkpoint_path.format(fold=fold_suffix),\n",
    "                monitor='val_loss',\n",
    "                save_best_only=True,\n",
    "                verbose=1\n",
    "            ),\n",
    "            TensorBoard(\n",
    "                log_dir=os.path.join(self.log_dir, f'fold{fold_suffix}'),\n",
    "                histogram_freq=1,\n",
    "                write_graph=True\n",
    "            ),\n",
    "            CSVLogger(\n",
    "                os.path.join(self.log_dir, f'training_log{fold_suffix}.csv'),\n",
    "                separator=',',\n",
    "                append=False\n",
    "            )\n",
    "        ]\n",
    "    \n",
    "    def cross_validate_hyperparameters(self, hp, x, y, batch_size):\n",
    "        \"\"\"Perform k-fold cross-validation for a set of hyperparameters\"\"\"\n",
    "        kf = KFold(n_splits=self.n_folds, shuffle=True, random_state=42)\n",
    "        scores = []\n",
    "        \n",
    "        for fold, (train_idx, val_idx) in enumerate(kf.split(x)):\n",
    "            x_train_fold = x[train_idx]\n",
    "            y_train_fold = y[train_idx]\n",
    "            x_val_fold = x[val_idx]\n",
    "            y_val_fold = y[val_idx]\n",
    "            \n",
    "            model = self.build_model(hp)\n",
    "            checkpoint_path = os.path.join(self.log_dir, 'model_cv_{fold}.h5')\n",
    "            callbacks = self.get_callbacks(checkpoint_path, fold)\n",
    "            \n",
    "            model.fit(\n",
    "                x_train_fold,\n",
    "                y_train_fold,\n",
    "                epochs=100,\n",
    "                batch_size=batch_size,\n",
    "                validation_data=(x_val_fold, y_val_fold),\n",
    "                callbacks=callbacks,\n",
    "                verbose=0\n",
    "            )\n",
    "            \n",
    "            scores.append(model.evaluate(x_val_fold, y_val_fold)[0])\n",
    "        \n",
    "        return np.mean(scores)\n",
    "    \n",
    "    def search(self, training_features, training_labels, validation_data, batch_size=24):\n",
    "        checkpoint_path = os.path.join(self.log_dir, 'best_model.keras')\n",
    "        \n",
    "        # Initialize Bayesian Optimization tuner\n",
    "        tuner = kt.BayesianOptimization(\n",
    "            self.build_model,\n",
    "            objective=kt.Objective('val_loss', direction='min'),\n",
    "            max_trials=self.max_trials,\n",
    "            directory=self.log_dir,\n",
    "            project_name=self.project_name,\n",
    "            overwrite=True\n",
    "        )\n",
    "        \n",
    "        # Print search space summary\n",
    "        tuner.search_space_summary()\n",
    "        \n",
    "        # Implement early stopping for the hyperparameter search\n",
    "        stop_early = EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            min_delta=1e-4,\n",
    "            patience=3\n",
    "        )\n",
    "        \n",
    "        # Search with cross-validation\n",
    "        tuner.search(\n",
    "            training_features,\n",
    "            training_labels,\n",
    "            epochs=100,\n",
    "            batch_size=batch_size,\n",
    "            validation_data=validation_data,\n",
    "            callbacks=self.get_callbacks(checkpoint_path),\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Get best hyperparameters\n",
    "        best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "        \n",
    "        # Print detailed results\n",
    "        print(\"\\nBest Hyperparameters:\")\n",
    "        for param, value in best_hps.values.items():\n",
    "            print(f\"{param}: {value}\")\n",
    "        \n",
    "        # Build and train final model with best parameters\n",
    "        best_model = tuner.hypermodel.build(best_hps)\n",
    "        \n",
    "        history = best_model.fit(\n",
    "            training_features,\n",
    "            training_labels,\n",
    "            epochs=100,\n",
    "            batch_size=batch_size,\n",
    "            validation_data=validation_data,\n",
    "            callbacks=self.get_callbacks(checkpoint_path),\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        return best_model, history, best_hps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 00m 57s]\n",
      "val_loss: 1213.5301513671875\n",
      "\n",
      "Best val_loss So Far: 938.81689453125\n",
      "Total elapsed time: 00h 05m 33s\n",
      "\n",
      "Best Hyperparameters:\n",
      "dropout1: 0.2\n",
      "units1: 32\n",
      "activation1: relu\n",
      "l2_reg1: 0.009925530546395422\n",
      "units2: 64\n",
      "activation2: relu\n",
      "l2_reg2: 0.00458502351256499\n",
      "base_learning_rate: 0.00033587186424780983\n",
      "beta_1: 0.9719311428881368\n",
      "beta_2: 0.9153644416804168\n",
      "weight_decay: 0.006404687722635315\n",
      "Epoch 1/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 15734.9111 - mape: 99.2074\n",
      "Epoch 1: val_loss improved from inf to 17018.51562, saving model to logs/20250107-222014/best_model.keras\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 33ms/step - loss: 15739.0518 - mape: 99.2016 - val_loss: 17018.5156 - val_mape: 98.2245 - learning_rate: 3.3587e-04\n",
      "Epoch 2/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 15862.6426 - mape: 97.7052\n",
      "Epoch 2: val_loss improved from 17018.51562 to 16549.32422, saving model to logs/20250107-222014/best_model.keras\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 15858.0566 - mape: 97.6984 - val_loss: 16549.3242 - val_mape: 96.5872 - learning_rate: 3.3587e-04\n",
      "Epoch 3/100\n",
      "\u001b[1m59/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 14968.5947 - mape: 95.9537\n",
      "Epoch 3: val_loss improved from 16549.32422 to 15948.93945, saving model to logs/20250107-222014/best_model.keras\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 14973.0625 - mape: 95.9248 - val_loss: 15948.9395 - val_mape: 94.4436 - learning_rate: 3.3587e-04\n",
      "Epoch 4/100\n",
      "\u001b[1m53/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 14782.0889 - mape: 93.5595\n",
      "Epoch 4: val_loss improved from 15948.93945 to 15145.72168, saving model to logs/20250107-222014/best_model.keras\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 14740.0781 - mape: 93.4665 - val_loss: 15145.7217 - val_mape: 91.4878 - learning_rate: 3.3587e-04\n",
      "Epoch 5/100\n",
      "\u001b[1m52/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 13932.5547 - mape: 90.4710\n",
      "Epoch 5: val_loss improved from 15145.72168 to 14081.22852, saving model to logs/20250107-222014/best_model.keras\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 13874.8496 - mape: 90.3221 - val_loss: 14081.2285 - val_mape: 87.4227 - learning_rate: 3.3587e-04\n",
      "Epoch 6/100\n",
      "\u001b[1m59/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 12852.1553 - mape: 85.8759\n",
      "Epoch 6: val_loss improved from 14081.22852 to 12733.50684, saving model to logs/20250107-222014/best_model.keras\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 12833.3203 - mape: 85.8138 - val_loss: 12733.5068 - val_mape: 82.0084 - learning_rate: 3.3587e-04\n",
      "Epoch 7/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 11971.7031 - mape: 80.4189\n",
      "Epoch 7: val_loss improved from 12733.50684 to 11065.66699, saving model to logs/20250107-222014/best_model.keras\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 11958.2002 - mape: 80.3916 - val_loss: 11065.6670 - val_mape: 74.8193 - learning_rate: 3.3587e-04\n",
      "Epoch 8/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 9857.4883 - mape: 72.3641\n",
      "Epoch 8: val_loss improved from 11065.66699 to 9117.17676, saving model to logs/20250107-222014/best_model.keras\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 9851.2119 - mape: 72.3401 - val_loss: 9117.1768 - val_mape: 65.5343 - learning_rate: 3.3587e-04\n",
      "Epoch 9/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 7625.3867 - mape: 62.7301\n",
      "Epoch 9: val_loss improved from 9117.17676 to 6967.95654, saving model to logs/20250107-222014/best_model.keras\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 7624.9072 - mape: 62.6984 - val_loss: 6967.9565 - val_mape: 53.8019 - learning_rate: 3.3587e-04\n",
      "Epoch 10/100\n",
      "\u001b[1m57/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 6461.0762 - mape: 52.8340\n",
      "Epoch 10: val_loss improved from 6967.95654 to 4763.08496, saving model to logs/20250107-222014/best_model.keras\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 6398.0977 - mape: 52.5612 - val_loss: 4763.0850 - val_mape: 40.8580 - learning_rate: 3.3587e-04\n",
      "Epoch 11/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 4146.2480 - mape: 40.3335\n",
      "Epoch 11: val_loss improved from 4763.08496 to 2851.64917, saving model to logs/20250107-222014/best_model.keras\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 4139.0864 - mape: 40.2933 - val_loss: 2851.6492 - val_mape: 29.1466 - learning_rate: 3.3587e-04\n",
      "Epoch 12/100\n",
      "\u001b[1m59/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2444.5344 - mape: 32.4139\n",
      "Epoch 12: val_loss improved from 2851.64917 to 1793.76685, saving model to logs/20250107-222014/best_model.keras\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - loss: 2439.4746 - mape: 32.3513 - val_loss: 1793.7668 - val_mape: 26.6162 - learning_rate: 3.3587e-04\n",
      "Epoch 13/100\n",
      "\u001b[1m60/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1886.0525 - mape: 31.5161\n",
      "Epoch 13: val_loss improved from 1793.76685 to 1784.18591, saving model to logs/20250107-222014/best_model.keras\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 1889.4117 - mape: 31.5611 - val_loss: 1784.1859 - val_mape: 29.6226 - learning_rate: 3.3587e-04\n",
      "Epoch 14/100\n",
      "\u001b[1m58/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1873.3335 - mape: 31.8213\n",
      "Epoch 14: val_loss improved from 1784.18591 to 1630.29468, saving model to logs/20250107-222014/best_model.keras\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 1875.9020 - mape: 31.8427 - val_loss: 1630.2947 - val_mape: 25.4377 - learning_rate: 3.3587e-04\n",
      "Epoch 15/100\n",
      "\u001b[1m59/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1694.4336 - mape: 28.5375\n",
      "Epoch 15: val_loss improved from 1630.29468 to 1556.06323, saving model to logs/20250107-222014/best_model.keras\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 1698.2212 - mape: 28.5746 - val_loss: 1556.0632 - val_mape: 24.8257 - learning_rate: 3.3587e-04\n",
      "Epoch 16/100\n",
      "\u001b[1m51/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1799.6356 - mape: 29.5179\n",
      "Epoch 16: val_loss improved from 1556.06323 to 1495.72302, saving model to logs/20250107-222014/best_model.keras\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 1792.7050 - mape: 29.5207 - val_loss: 1495.7230 - val_mape: 25.4139 - learning_rate: 3.3587e-04\n",
      "Epoch 17/100\n",
      "\u001b[1m53/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1796.9518 - mape: 27.9679\n",
      "Epoch 17: val_loss improved from 1495.72302 to 1435.96973, saving model to logs/20250107-222014/best_model.keras\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 1768.8784 - mape: 28.0564 - val_loss: 1435.9697 - val_mape: 24.0638 - learning_rate: 3.3587e-04\n",
      "Epoch 18/100\n",
      "\u001b[1m54/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1650.2831 - mape: 28.3711\n",
      "Epoch 18: val_loss improved from 1435.96973 to 1388.41284, saving model to logs/20250107-222014/best_model.keras\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 1640.0745 - mape: 28.2895 - val_loss: 1388.4128 - val_mape: 23.0761 - learning_rate: 3.3587e-04\n",
      "Epoch 19/100\n",
      "\u001b[1m58/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1640.1437 - mape: 26.8638\n",
      "Epoch 19: val_loss improved from 1388.41284 to 1346.63074, saving model to logs/20250107-222014/best_model.keras\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 1638.8202 - mape: 26.9224 - val_loss: 1346.6307 - val_mape: 23.6102 - learning_rate: 3.3587e-04\n",
      "Epoch 20/100\n",
      "\u001b[1m54/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1551.4552 - mape: 27.7958\n",
      "Epoch 20: val_loss improved from 1346.63074 to 1305.78333, saving model to logs/20250107-222014/best_model.keras\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 1537.6906 - mape: 27.6959 - val_loss: 1305.7833 - val_mape: 22.7017 - learning_rate: 3.3587e-04\n",
      "Epoch 21/100\n",
      "\u001b[1m51/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1448.8395 - mape: 27.4009\n",
      "Epoch 21: val_loss improved from 1305.78333 to 1272.12366, saving model to logs/20250107-222014/best_model.keras\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 1456.5594 - mape: 27.3702 - val_loss: 1272.1237 - val_mape: 22.0654 - learning_rate: 3.3587e-04\n",
      "Epoch 22/100\n",
      "\u001b[1m58/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1412.1024 - mape: 26.5105\n",
      "Epoch 22: val_loss improved from 1272.12366 to 1241.23145, saving model to logs/20250107-222014/best_model.keras\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1414.6530 - mape: 26.5205 - val_loss: 1241.2314 - val_mape: 21.7485 - learning_rate: 3.3587e-04\n",
      "Epoch 23/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1260.9265 - mape: 26.0432\n",
      "Epoch 23: val_loss improved from 1241.23145 to 1209.60291, saving model to logs/20250107-222014/best_model.keras\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 1263.1835 - mape: 26.0366 - val_loss: 1209.6029 - val_mape: 21.5223 - learning_rate: 3.3587e-04\n",
      "Epoch 24/100\n",
      "\u001b[1m51/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1298.2994 - mape: 27.0909\n",
      "Epoch 24: val_loss improved from 1209.60291 to 1184.82751, saving model to logs/20250107-222014/best_model.keras\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 1305.6577 - mape: 26.9425 - val_loss: 1184.8275 - val_mape: 21.4782 - learning_rate: 3.3587e-04\n",
      "Epoch 25/100\n",
      "\u001b[1m56/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1440.3160 - mape: 26.4746\n",
      "Epoch 25: val_loss improved from 1184.82751 to 1163.32434, saving model to logs/20250107-222014/best_model.keras\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 1431.2867 - mape: 26.3967 - val_loss: 1163.3243 - val_mape: 21.0606 - learning_rate: 3.3587e-04\n",
      "Epoch 26/100\n",
      "\u001b[1m53/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1333.2500 - mape: 25.6573\n",
      "Epoch 26: val_loss improved from 1163.32434 to 1148.82043, saving model to logs/20250107-222014/best_model.keras\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 1335.5365 - mape: 25.5496 - val_loss: 1148.8204 - val_mape: 20.5103 - learning_rate: 3.3587e-04\n",
      "Epoch 27/100\n",
      "\u001b[1m56/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1334.7227 - mape: 26.3531\n",
      "Epoch 27: val_loss improved from 1148.82043 to 1127.65356, saving model to logs/20250107-222014/best_model.keras\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 1330.7458 - mape: 26.2396 - val_loss: 1127.6536 - val_mape: 20.6671 - learning_rate: 3.3587e-04\n",
      "Epoch 28/100\n",
      "\u001b[1m19/61\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1502.4739 - mape: 25.2549\n",
      "Epoch 28: val_loss improved from 1127.65356 to 1111.37170, saving model to logs/20250107-222014/best_model.keras\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m-2s\u001b[0m -29950us/step - loss: 1401.2189 - mape: 25.2859 - val_loss: 1111.3717 - val_mape: 20.2816 - learning_rate: 3.3587e-04\n",
      "Epoch 29/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1296.8459 - mape: 24.4039\n",
      "Epoch 29: val_loss improved from 1111.37170 to 1101.04871, saving model to logs/20250107-222014/best_model.keras\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 1296.8639 - mape: 24.4160 - val_loss: 1101.0487 - val_mape: 19.9772 - learning_rate: 3.3587e-04\n",
      "Epoch 30/100\n",
      "\u001b[1m52/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1309.6266 - mape: 25.0355\n",
      "Epoch 30: val_loss improved from 1101.04871 to 1087.37964, saving model to logs/20250107-222014/best_model.keras\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 1309.6594 - mape: 25.0366 - val_loss: 1087.3796 - val_mape: 20.0372 - learning_rate: 3.3587e-04\n",
      "Epoch 31/100\n",
      "\u001b[1m55/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1268.0583 - mape: 23.0567\n",
      "Epoch 31: val_loss improved from 1087.37964 to 1077.12952, saving model to logs/20250107-222014/best_model.keras\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 1261.4757 - mape: 23.1778 - val_loss: 1077.1295 - val_mape: 19.8642 - learning_rate: 3.3587e-04\n",
      "Epoch 32/100\n",
      "\u001b[1m53/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1187.8007 - mape: 24.2332\n",
      "Epoch 32: val_loss improved from 1077.12952 to 1068.95068, saving model to logs/20250107-222014/best_model.keras\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 1186.5813 - mape: 24.0937 - val_loss: 1068.9507 - val_mape: 19.7779 - learning_rate: 3.3587e-04\n",
      "Epoch 33/100\n",
      "\u001b[1m53/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1367.5299 - mape: 23.5056\n",
      "Epoch 33: val_loss improved from 1068.95068 to 1054.61365, saving model to logs/20250107-222014/best_model.keras\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 1350.7841 - mape: 23.5639 - val_loss: 1054.6136 - val_mape: 19.9865 - learning_rate: 3.3587e-04\n",
      "Epoch 34/100\n",
      "\u001b[1m56/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1164.6538 - mape: 22.3219\n",
      "Epoch 34: val_loss improved from 1054.61365 to 1053.32434, saving model to logs/20250107-222014/best_model.keras\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 1164.0618 - mape: 22.4296 - val_loss: 1053.3243 - val_mape: 19.4078 - learning_rate: 3.3587e-04\n",
      "Epoch 35/100\n",
      "\u001b[1m58/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1125.7311 - mape: 22.5082\n",
      "Epoch 35: val_loss improved from 1053.32434 to 1039.32593, saving model to logs/20250107-222014/best_model.keras\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 1125.1021 - mape: 22.5297 - val_loss: 1039.3259 - val_mape: 19.8358 - learning_rate: 3.3587e-04\n",
      "Epoch 36/100\n",
      "\u001b[1m57/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 1184.5425 - mape: 24.3092\n",
      "Epoch 36: val_loss improved from 1039.32593 to 1035.96130, saving model to logs/20250107-222014/best_model.keras\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 1180.9756 - mape: 24.2769 - val_loss: 1035.9613 - val_mape: 19.5217 - learning_rate: 3.3587e-04\n",
      "Epoch 37/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1232.1803 - mape: 24.4818\n",
      "Epoch 37: val_loss did not improve from 1035.96130\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 1231.5724 - mape: 24.4640 - val_loss: 1036.1082 - val_mape: 19.1840 - learning_rate: 3.3587e-04\n",
      "Epoch 38/100\n",
      "\u001b[1m54/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1115.9056 - mape: 23.7907\n",
      "Epoch 38: val_loss improved from 1035.96130 to 1021.96716, saving model to logs/20250107-222014/best_model.keras\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 1122.7543 - mape: 23.7681 - val_loss: 1021.9672 - val_mape: 19.4788 - learning_rate: 3.3587e-04\n",
      "Epoch 39/100\n",
      "\u001b[1m53/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1155.9883 - mape: 24.1461\n",
      "Epoch 39: val_loss improved from 1021.96716 to 1016.32458, saving model to logs/20250107-222014/best_model.keras\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 1151.4215 - mape: 24.0848 - val_loss: 1016.3246 - val_mape: 19.3312 - learning_rate: 3.3587e-04\n",
      "Epoch 40/100\n",
      "\u001b[1m53/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1001.6226 - mape: 22.8634\n",
      "Epoch 40: val_loss improved from 1016.32458 to 1012.85468, saving model to logs/20250107-222014/best_model.keras\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 1015.2316 - mape: 22.9280 - val_loss: 1012.8547 - val_mape: 19.2620 - learning_rate: 3.3587e-04\n",
      "Epoch 41/100\n",
      "\u001b[1m52/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 1091.7043 - mape: 22.1988\n",
      "Epoch 41: val_loss did not improve from 1012.85468\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 1095.4109 - mape: 22.3077 - val_loss: 1014.1472 - val_mape: 18.9760 - learning_rate: 3.3587e-04\n",
      "Epoch 42/100\n",
      "\u001b[1m56/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1273.5258 - mape: 24.2514\n",
      "Epoch 42: val_loss improved from 1012.85468 to 1009.02319, saving model to logs/20250107-222014/best_model.keras\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1263.0422 - mape: 24.1667 - val_loss: 1009.0232 - val_mape: 18.9675 - learning_rate: 3.3587e-04\n",
      "Epoch 43/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1144.9142 - mape: 22.6647\n",
      "Epoch 43: val_loss improved from 1009.02319 to 998.41913, saving model to logs/20250107-222014/best_model.keras\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1144.7275 - mape: 22.6707 - val_loss: 998.4191 - val_mape: 19.1186 - learning_rate: 3.3587e-04\n",
      "Epoch 44/100\n",
      "\u001b[1m58/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1076.3644 - mape: 22.3454\n",
      "Epoch 44: val_loss did not improve from 998.41913\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1078.7264 - mape: 22.3894 - val_loss: 1008.6200 - val_mape: 18.6997 - learning_rate: 3.3587e-04\n",
      "Epoch 45/100\n",
      "\u001b[1m55/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1036.5803 - mape: 21.1569\n",
      "Epoch 45: val_loss improved from 998.41913 to 991.60278, saving model to logs/20250107-222014/best_model.keras\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1042.2086 - mape: 21.3050 - val_loss: 991.6028 - val_mape: 18.8749 - learning_rate: 3.3587e-04\n",
      "Epoch 46/100\n",
      "\u001b[1m55/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1121.5900 - mape: 22.2983\n",
      "Epoch 46: val_loss improved from 991.60278 to 985.36871, saving model to logs/20250107-222014/best_model.keras\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 1117.6576 - mape: 22.3398 - val_loss: 985.3687 - val_mape: 18.8265 - learning_rate: 3.3587e-04\n",
      "Epoch 47/100\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 876.5820 - mape: 21.3292\n",
      "Epoch 47: val_loss improved from 985.36871 to 983.76855, saving model to logs/20250107-222014/best_model.keras\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 879.1554 - mape: 21.3383 - val_loss: 983.7686 - val_mape: 18.6820 - learning_rate: 3.3587e-04\n",
      "Epoch 48/100\n",
      "\u001b[1m59/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1119.2637 - mape: 22.9821\n",
      "Epoch 48: val_loss improved from 983.76855 to 975.40039, saving model to logs/20250107-222014/best_model.keras\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 1117.8622 - mape: 22.9549 - val_loss: 975.4004 - val_mape: 18.7403 - learning_rate: 3.3587e-04\n",
      "Epoch 49/100\n",
      "\u001b[1m59/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1011.6830 - mape: 21.7382\n",
      "Epoch 49: val_loss did not improve from 975.40039\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 1013.1893 - mape: 21.7557 - val_loss: 979.7744 - val_mape: 18.5420 - learning_rate: 3.3587e-04\n",
      "Epoch 50/100\n",
      "\u001b[1m53/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1082.2828 - mape: 21.9296\n",
      "Epoch 50: val_loss improved from 975.40039 to 967.49884, saving model to logs/20250107-222014/best_model.keras\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 1078.1862 - mape: 21.9589 - val_loss: 967.4988 - val_mape: 18.6943 - learning_rate: 3.3587e-04\n",
      "Epoch 51/100\n",
      "\u001b[1m51/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 950.0312 - mape: 22.1968\n",
      "Epoch 51: val_loss did not improve from 967.49884\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 965.5601 - mape: 22.1926 - val_loss: 977.2158 - val_mape: 18.4247 - learning_rate: 3.3587e-04\n",
      "Epoch 52/100\n",
      "\u001b[1m51/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 912.0801 - mape: 22.2223\n",
      "Epoch 52: val_loss improved from 967.49884 to 965.73846, saving model to logs/20250107-222014/best_model.keras\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 931.4562 - mape: 22.1968 - val_loss: 965.7385 - val_mape: 18.5716 - learning_rate: 3.3587e-04\n",
      "Epoch 53/100\n",
      "\u001b[1m55/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1046.9635 - mape: 21.8671\n",
      "Epoch 53: val_loss did not improve from 965.73846\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1050.2114 - mape: 21.9052 - val_loss: 971.0002 - val_mape: 18.3455 - learning_rate: 3.3587e-04\n",
      "Epoch 54/100\n",
      "\u001b[1m60/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1013.3579 - mape: 22.5633\n",
      "Epoch 54: val_loss improved from 965.73846 to 965.11676, saving model to logs/20250107-222014/best_model.keras\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 1014.2698 - mape: 22.5477 - val_loss: 965.1168 - val_mape: 18.3350 - learning_rate: 3.3587e-04\n",
      "Epoch 55/100\n",
      "\u001b[1m57/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 939.2662 - mape: 20.8697\n",
      "Epoch 55: val_loss improved from 965.11676 to 958.21033, saving model to logs/20250107-222014/best_model.keras\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 941.9134 - mape: 20.9278 - val_loss: 958.2103 - val_mape: 18.3694 - learning_rate: 3.3587e-04\n",
      "Epoch 56/100\n",
      "\u001b[1m59/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1036.3480 - mape: 22.4815\n",
      "Epoch 56: val_loss improved from 958.21033 to 956.28735, saving model to logs/20250107-222014/best_model.keras\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 1034.8754 - mape: 22.4567 - val_loss: 956.2874 - val_mape: 18.2701 - learning_rate: 3.3587e-04\n",
      "Epoch 57/100\n",
      "\u001b[1m55/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1000.4396 - mape: 22.4511\n",
      "Epoch 57: val_loss did not improve from 956.28735\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1004.1449 - mape: 22.3877 - val_loss: 961.7321 - val_mape: 18.1473 - learning_rate: 3.3587e-04\n",
      "Epoch 58/100\n",
      "\u001b[1m53/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 898.9510 - mape: 20.6925\n",
      "Epoch 58: val_loss improved from 956.28735 to 947.49963, saving model to logs/20250107-222014/best_model.keras\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 905.4311 - mape: 20.7901 - val_loss: 947.4996 - val_mape: 18.2774 - learning_rate: 3.3587e-04\n",
      "Epoch 59/100\n",
      "\u001b[1m60/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1041.6763 - mape: 22.2082\n",
      "Epoch 59: val_loss did not improve from 947.49963\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1040.5270 - mape: 22.1989 - val_loss: 966.8553 - val_mape: 18.1318 - learning_rate: 3.3587e-04\n",
      "Epoch 60/100\n",
      "\u001b[1m59/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1003.7839 - mape: 22.2686\n",
      "Epoch 60: val_loss improved from 947.49963 to 943.77380, saving model to logs/20250107-222014/best_model.keras\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 1001.3223 - mape: 22.2417 - val_loss: 943.7738 - val_mape: 18.2942 - learning_rate: 3.3587e-04\n",
      "Epoch 61/100\n",
      "\u001b[1m55/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 949.7762 - mape: 22.8643\n",
      "Epoch 61: val_loss improved from 943.77380 to 937.84961, saving model to logs/20250107-222014/best_model.keras\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 955.0696 - mape: 22.7328 - val_loss: 937.8496 - val_mape: 18.2515 - learning_rate: 3.3587e-04\n",
      "Epoch 62/100\n",
      "\u001b[1m51/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1060.7125 - mape: 21.5535\n",
      "Epoch 62: val_loss did not improve from 937.84961\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1048.0278 - mape: 21.5916 - val_loss: 944.2782 - val_mape: 18.1081 - learning_rate: 3.3587e-04\n",
      "Epoch 63/100\n",
      "\u001b[1m55/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1056.1628 - mape: 20.9556\n",
      "Epoch 63: val_loss did not improve from 937.84961\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1045.2396 - mape: 20.9829 - val_loss: 951.8899 - val_mape: 18.0347 - learning_rate: 3.3587e-04\n",
      "Epoch 64/100\n",
      "\u001b[1m59/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 946.7914 - mape: 21.1884\n",
      "Epoch 64: ReduceLROnPlateau reducing learning rate to 0.00016793592658359557.\n",
      "\n",
      "Epoch 64: val_loss did not improve from 937.84961\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 947.2730 - mape: 21.1846 - val_loss: 955.1568 - val_mape: 18.0183 - learning_rate: 3.3587e-04\n",
      "Epoch 65/100\n",
      "\u001b[1m57/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1065.0745 - mape: 21.4461\n",
      "Epoch 65: val_loss did not improve from 937.84961\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1057.0292 - mape: 21.4013 - val_loss: 943.8373 - val_mape: 18.0966 - learning_rate: 1.6794e-04\n",
      "Epoch 66/100\n",
      "\u001b[1m50/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1085.1841 - mape: 22.5519\n",
      "Epoch 66: val_loss did not improve from 937.84961\n",
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - loss: 1063.3147 - mape: 22.4273 - val_loss: 937.8824 - val_mape: 18.1183 - learning_rate: 1.6794e-04\n",
      "Epoch 66: early stopping\n",
      "Restoring model weights from the end of the best epoch: 61.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tuner = BayesianNeuralTuner(\n",
    "    input_shape=(training_features.shape[1],),\n",
    "    max_trials=10,\n",
    "    n_folds=3\n",
    ")\n",
    "\n",
    "best_model, history, best_hps = tuner.search(\n",
    "    training_features,\n",
    "    training_labels,\n",
    "    validation_data=(validation_features, validation_labels)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'early_stopping' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train the final model\u001b[39;00m\n\u001b[1;32m      2\u001b[0m history \u001b[38;5;241m=\u001b[39m best_model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m      3\u001b[0m     training_features,\n\u001b[1;32m      4\u001b[0m     training_labels,\n\u001b[1;32m      5\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,\n\u001b[1;32m      6\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m24\u001b[39m,\n\u001b[1;32m      7\u001b[0m     validation_data\u001b[38;5;241m=\u001b[39m(validation_features, validation_labels),\n\u001b[0;32m----> 8\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[\u001b[43mearly_stopping\u001b[49m]\n\u001b[1;32m      9\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'early_stopping' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train the final model\n",
    "history = best_model.fit(\n",
    "    training_features,\n",
    "    training_labels,\n",
    "    epochs=100,\n",
    "    batch_size=24,\n",
    "    validation_data=(validation_features, validation_labels),\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the trained Model\n",
    "model.save(\"python_model_Brot.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m46/46\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "MAPE on the Training Data: 18.66%\n",
      "MAPE on the Validation Data: 18.25%\n"
     ]
    }
   ],
   "source": [
    "# Making Predictions and Evaluating the Model\n",
    "import numpy as np\n",
    "\n",
    "def mape(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    non_zero_mask = y_true != 0\n",
    "    return np.mean(np.abs((y_true[non_zero_mask] - y_pred[non_zero_mask]) / y_true[non_zero_mask])) * 100\n",
    "\n",
    "training_predictions = best_model.predict(training_features)\n",
    "validation_predictions = best_model.predict(validation_features)\n",
    "print(f\"MAPE on the Training Data: {mape(training_labels, training_predictions):.2f}%\")\n",
    "print(f\"MAPE on the Validation Data: {mape(validation_labels, validation_predictions):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Training History\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss During Training')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plotting Training History (ab der 5. Epoche)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Schneide die Verlaufsdaten ab der 5. Epoche\n",
    "epochs_to_plot = range(5, len(history.history['loss']) + 1)\n",
    "training_loss = history.history['loss'][4:]\n",
    "validation_loss = history.history['val_loss'][4:]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(epochs_to_plot, training_loss, label='Training Loss')\n",
    "plt.plot(epochs_to_plot, validation_loss, label='Validation Loss')\n",
    "plt.title('Model Loss During Training (Starting from Epoch 5)')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***VORHERSAGE TESTZEITRAUM***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = model.predict(test_features)\n",
    "#print(\"Predictions for test data:\", test_predictions[:5])  # Display first 5 predictions\n",
    "# Größe des Arrays anzeigen\n",
    "print(\"Größe des Arrays (shape):\", test_predictions.shape)\n",
    "test_predictions = pd.DataFrame(test_predictions)\n",
    "test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = pd.read_csv(\"/workspaces/bakery_sales_prediction/0_DataPreparation/00_data/data_with_lag/Testdaten_with_lag.csv\")\n",
    "data_test = data_test[data_test[\"Warengruppe_1\"] == 1]\n",
    "data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstelle Ziel csv Datei\n",
    "\n",
    "predictions_df = pd.DataFrame({\n",
    "    'id': data_test['id'].values,\n",
    "    'Umsatz': test_predictions[0].values\n",
    "}).reset_index(drop=True)\n",
    "\n",
    "# Save to a CSV file\n",
    "#output_csv_path = \"/workspaces/bakery_sales_prediction/3_Model/02_Franz/V3_lag/Umsatzvorhersage_Broetchen_Hochladeformat.csv\"\n",
    "\n",
    "predictions_df.to_csv('Brot_Umsatzvorhersage.csv', index=False)\n",
    "print(f\"CSV Datei erstellt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
