{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d-3CyESydUAq"
   },
   "source": [
    "## Importing the Data\n",
    "\n",
    "We will start by importing the data and taking a look at the first few rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "zW1yixRmpNCD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Training features dimensions: (7493, 46)\n",
      "Loaded Validation features dimensions: (1841, 46)\n",
      "Loaded Test features dimensions: (1830, 46)\n",
      "\n",
      "Loaded Training labels dimensions: (7493, 1)\n",
      "Loaded Validation labels dimensions: (1841, 1)\n",
      "\n",
      "First few rows of loaded training features:\n",
      "   Warengruppe_1  Warengruppe_2  Warengruppe_3  Warengruppe_4  Warengruppe_5  \\\n",
      "0              1              0              0              0              0   \n",
      "1              1              0              0              0              0   \n",
      "2              1              0              0              0              0   \n",
      "3              1              0              0              0              0   \n",
      "4              1              0              0              0              0   \n",
      "\n",
      "   Warengruppe_6  Temp_Very_Cold  Temp_Cold  Temp_Mild  Temp_Warm  ...  Markt  \\\n",
      "0              0               0          0          1          0  ...      0   \n",
      "1              0               0          0          1          0  ...      0   \n",
      "2              0               0          0          0          1  ...      0   \n",
      "3              0               0          0          1          0  ...      0   \n",
      "4              0               0          0          1          0  ...      0   \n",
      "\n",
      "   Ostertag  Silvester  wetter_sehr_schön  wetter_sehr_schlecht  Frühling  \\\n",
      "0         0          0                  0                     0         0   \n",
      "1         0          0                  0                     0         0   \n",
      "2         0          0                  0                     1         0   \n",
      "3         0          0                  0                     0         0   \n",
      "4         0          0                  0                     0         0   \n",
      "\n",
      "   Sommer  Herbst  Winter  Werktag  \n",
      "0       1       0       0        1  \n",
      "1       1       0       0        1  \n",
      "2       1       0       0        1  \n",
      "3       1       0       0        1  \n",
      "4       1       0       0        1  \n",
      "\n",
      "[5 rows x 46 columns]\n",
      "\n",
      "First few rows of loaded training labels:\n",
      "   Warengruppe_1  Warengruppe_2  Warengruppe_3  Warengruppe_4  Warengruppe_5  \\\n",
      "0              1              0              0              0              0   \n",
      "1              1              0              0              0              0   \n",
      "2              1              0              0              0              0   \n",
      "3              1              0              0              0              0   \n",
      "4              1              0              0              0              0   \n",
      "\n",
      "   Warengruppe_6  Temp_Very_Cold  Temp_Cold  Temp_Mild  Temp_Warm  ...  Markt  \\\n",
      "0              0               0          0          0          1  ...      0   \n",
      "1              0               0          0          0          0  ...      0   \n",
      "2              0               0          0          0          0  ...      0   \n",
      "3              0               0          0          0          0  ...      0   \n",
      "4              0               0          0          0          1  ...      0   \n",
      "\n",
      "   Ostertag  Silvester  wetter_sehr_schön  wetter_sehr_schlecht  Frühling  \\\n",
      "0         0          0                  1                     0         0   \n",
      "1         0          0                  1                     0         0   \n",
      "2         0          0                  1                     0         0   \n",
      "3         0          0                  1                     0         0   \n",
      "4         0          0                  0                     0         0   \n",
      "\n",
      "   Sommer  Herbst  Winter  Werktag  \n",
      "0       1       0       0        1  \n",
      "1       1       0       0        1  \n",
      "2       1       0       0        1  \n",
      "3       1       0       0        0  \n",
      "4       1       0       0        0  \n",
      "\n",
      "[5 rows x 46 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Import the necessary libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Define the file paths\n",
    "subdirectory = \"pickle_data\"\n",
    "training_features_path = f\"{subdirectory}/training_features.pkl\"\n",
    "validation_features_path = f\"{subdirectory}/validation_features.pkl\"\n",
    "test_features_path = f\"{subdirectory}/test_features.pkl\"\n",
    "training_labels_path = f\"{subdirectory}/training_labels.pkl\"\n",
    "validation_labels_path = f\"{subdirectory}/validation_labels.pkl\"\n",
    "#test_labels_path = f\"{subdirectory}/test_labels.pkl\"\n",
    "\n",
    "# Read the pickle files\n",
    "training_features = pd.read_pickle(training_features_path)\n",
    "validation_features = pd.read_pickle(validation_features_path)\n",
    "test_features = pd.read_pickle(test_features_path)\n",
    "training_labels = pd.read_pickle(training_labels_path)\n",
    "validation_labels = pd.read_pickle(validation_labels_path)\n",
    "#test_labels = pd.read_pickle(test_labels_path)\n",
    "\n",
    "# Verify the loaded data by printing their shapes and a few rows\n",
    "print(\"Loaded Training features dimensions:\", training_features.shape)\n",
    "print(\"Loaded Validation features dimensions:\", validation_features.shape)\n",
    "print(\"Loaded Test features dimensions:\", test_features.shape)\n",
    "print()\n",
    "print(\"Loaded Training labels dimensions:\", training_labels.shape)\n",
    "print(\"Loaded Validation labels dimensions:\", validation_labels.shape)\n",
    "#print(\"Loaded Test labels dimensions:\", test_labels.shape)\n",
    "print()\n",
    "\n",
    "print(\"First few rows of loaded training features:\")\n",
    "print(training_features.head())\n",
    "print()\n",
    "print(\"First few rows of loaded training labels:\")\n",
    "print(test_features.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QEn1WecUkUvY"
   },
   "source": [
    "## Defining the Neural Network\n",
    "\n",
    "Now, let's define our neural network. We are using a Sequential model definition from Keras with batch normalization and dense layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NIVMJ2ISYuJ1",
    "outputId": "fa361fbe-9c4b-412a-abff-80e92f29be03"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InputLayer, Dense, BatchNormalization\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Adam\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import InputLayer, Dense, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model = Sequential([\n",
    "  InputLayer(shape=(training_features.shape[1], )),\n",
    "  Dense(40, activation='relu'),\n",
    "  Dense(40, activation='relu'),\n",
    "  Dense(20, activation='relu'),\n",
    "  Dense(1)\n",
    "])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GHLvD-uEkbJI"
   },
   "source": [
    "## Compiling and Training the Model\n",
    "\n",
    "We will compile the model using Mean Squared Error (MSE) as the loss function and Adam optimizer. The model is then trained using the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3C9DP_BFZBdd",
    "outputId": "94db16e9-5d5a-4873-adc7-067770255a37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1735600297.138847   19565 service.cc:148] XLA service 0x7f8d5c0046a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1735600297.138929   19565 service.cc:156]   StreamExecutor device (0): NVIDIA GeForce RTX 4060 Laptop GPU, Compute Capability 8.9\n",
      "2024-12-31 00:11:37.157430: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1735600297.237530   19565 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 54/235\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 75069.9688"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1735600298.654216   19565 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - loss: 56687.3633 - val_loss: 11600.4600\n",
      "Epoch 2/20\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 12577.8760 - val_loss: 4716.7861\n",
      "Epoch 3/20\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 5928.1660 - val_loss: 3669.2322\n",
      "Epoch 4/20\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 5281.9746 - val_loss: 3393.5281\n",
      "Epoch 5/20\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 3770.9749 - val_loss: 3168.5264\n",
      "Epoch 6/20\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 3765.0410 - val_loss: 3107.3027\n",
      "Epoch 7/20\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 4784.8237 - val_loss: 3077.8809\n",
      "Epoch 8/20\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 2723.2693 - val_loss: 3346.9236\n",
      "Epoch 9/20\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 3394.1865 - val_loss: 3510.0464\n",
      "Epoch 10/20\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 3168.1125 - val_loss: 3011.1843\n",
      "Epoch 11/20\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 3501.1052 - val_loss: 3070.9919\n",
      "Epoch 12/20\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 3182.8206 - val_loss: 3618.5894\n",
      "Epoch 13/20\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 2393.4810 - val_loss: 2963.2744\n",
      "Epoch 14/20\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 2842.9819 - val_loss: 3009.8508\n",
      "Epoch 15/20\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 2866.7803 - val_loss: 3004.4033\n",
      "Epoch 16/20\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 2989.1055 - val_loss: 3610.6516\n",
      "Epoch 17/20\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 2834.4185 - val_loss: 2958.9270\n",
      "Epoch 18/20\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - loss: 2213.1450 - val_loss: 3211.6077\n",
      "Epoch 19/20\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 3135.0659 - val_loss: 2872.1819\n",
      "Epoch 20/20\n",
      "\u001b[1m235/235\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 2504.5125 - val_loss: 3073.9050\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"mse\", optimizer=Adam(learning_rate=0.001))\n",
    "\n",
    "history = model.fit(training_features, training_labels, epochs=20,\n",
    "                    validation_data=(validation_features, validation_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0S-qupIYn_7V"
   },
   "source": [
    "## Saving the Trained Model\n",
    "\n",
    "After training, it's a good practice to save the model for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gItgvuncoD_9",
    "outputId": "8b50b026-9136-4ade-fa47-71c18418fb81"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save(\"python_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XeywUAjmoGP1"
   },
   "source": [
    "## Plotting Training History\n",
    "\n",
    "Visualizing the training and validation loss can help us understand the model's performance over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 531
    },
    "id": "OJjar_X8oL4t",
    "outputId": "59d7a280-84db-474c-da8a-d77998193af7"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining Loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss During Training')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cQRMNHFOoJyO"
   },
   "source": [
    "## Making Predictions and Evaluating the Model\n",
    "\n",
    "Let's use the model to make predictions on our training and validation sets and evaluate the model's performance using Mean Absolute Percentage Error (MAPE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xUyG8jG5oQxy",
    "outputId": "a9710306-04bb-4e93-8b80-0a17db65c665"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def mape(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    non_zero_mask = y_true != 0\n",
    "    return np.mean(np.abs((y_true[non_zero_mask] - y_pred[non_zero_mask]) / y_true[non_zero_mask])) * 100\n",
    "\n",
    "training_predictions = model.predict(training_features)\n",
    "validation_predictions = model.predict(validation_features)\n",
    "print(f\"MAPE on the Training Data: {mape(training_labels, training_predictions):.2f}%\")\n",
    "print(f\"MAPE on the Validation Data: {mape(validation_labels, validation_predictions):.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1pzsqOgeoWvT"
   },
   "source": [
    "## Visualizing Predictions vs Actual Values\n",
    "\n",
    "Visualizing the predicted versus actual values can provide insights into the model's accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0y6IKrzMoZr3"
   },
   "outputs": [],
   "source": [
    "def plot_predictions(data, title):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(data['actual'], label='Actual Values', color='red')\n",
    "    plt.plot(data['prediction'], label='Predicted Values', color='blue')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Zeit')\n",
    "    plt.ylabel('Umsatz')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Ensure that training_predictions, validation_predictions, training_labels, and validation_labels are numpy arrays\n",
    "training_predictions = np.array(training_predictions).flatten()\n",
    "validation_predictions = np.array(validation_predictions).flatten()\n",
    "training_labels = np.array(training_labels).flatten()\n",
    "validation_labels = np.array(validation_labels).flatten()\n",
    "\n",
    "# print the type of the predictions\n",
    "print(type(training_predictions))\n",
    "print(type(validation_predictions))\n",
    "\n",
    "# Create DataFrames with 1-dimensional arrays\n",
    "data_train = pd.DataFrame({'prediction': training_predictions, 'actual': training_labels})\n",
    "data_validation = pd.DataFrame({'prediction': validation_predictions, 'actual': validation_labels})\n",
    "\n",
    "# Plot predictions\n",
    "plot_predictions(data_train.head(100), 'Predicted and Actual Values for the Training Data')\n",
    "plot_predictions(data_validation.head(100), 'Predicted and Actual Values for the Validation Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(test_features)\n",
    "pred = pd.DataFrame(pred)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the test-data\n",
    "\n",
    "test_data = pd.read_csv('/workspaces/bakery_sales_prediction/0_DataPreparation/00_data/Testdaten.csv')\n",
    "# test_data['VPI'] = vpi_scaler.transform(test_data[['VPI']])\n",
    "test_data\n",
    "\n",
    "# let's merge predictions with the 'id' column:\n",
    "predictions_df = pd.DataFrame({\n",
    "    'id': test_data['id'].values,\n",
    "    'Umsatz': pred[0].values\n",
    "}).reset_index(drop=True)\n",
    "\n",
    "predictions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Speichert den DataFrame als CSV-Datei für die Kaggle-Submission\n",
    "# index=False verhindert, dass eine zusätzliche Indexspalte geschrieben wird\n",
    "predictions_df.to_csv('01_DNN_predictions.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
